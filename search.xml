<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[海量数据去重]]></title>
    <url>%2F2019%2F01%2F30%2F2019-01-30_%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[前提 通过id去重，而不是整条数据 id由SnowFlake算法生成，参考之前的文章SnowFlake算法在数据链路中的应用 需求 在实时平台的各个环节中，由于网络或其他问题，有时会出现数据重复的情况，本质上是由于at least once保障机制造成的。例如 flume agent之间的数据传输，如果网络不稳定，有可能出现src_agent发送数据超时而导致重发，但实际上dest_agent已经收到，造成了数据重复 kafka producer发送数据且设置acks=all，在replication完成之间就由于超时而返回失败，如果retries不为0，那么重发之后数据也会有重复 通常我们会在业务端通过幂等性来保证数据的唯一性，比如Mysql的primary key，或者是HBase的rowkey。但在流式计算或某些存储介质中，没有办法天然的实现数据去重，这时就需要在数据计算/存储之前将重复的数据移除或忽略 思路 我司的实时数据都是通过Flume采集，并且通过SnowFlake算法给每条数据分配一个全局唯一长整型的id，这个id会被带到整条数据链路中，所以考虑开发一个去重模块，对实时数据进行预处理。又由于id是数字类型，可以考虑用BitSet进行存储以提高查询效率和减小开销，但java.util.BitSet的最大长度是Integer.MAX_VALUE(2GB)，再长的话内存开销就会非常巨大，所以需要对id进行分段存储 原始的id是由41位时间戳，8位机器信息和12位序列号组合而成 将时间戳拆分成秒和毫秒两部分 重新将各部分组合成新的key-value pair，秒数和机器信息拼接为一个long型的key，序列号和毫秒数拼接成一个int型的value。假设对n分钟内的数据进行过滤，则key的最大个数为n60256，value最大个数为4096*1000 将相同key的数据放到同一个BitSet中，并缓存到LoadingCache中 代码 注意：DuplicationEliminator中的常量必须与SnowFlake算法中一致，否则会解析错位 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import java.util.BitSet;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import org.apache.commons.lang3.tuple.Pair;import com.google.common.cache.CacheBuilder;import com.google.common.cache.CacheLoader;import com.google.common.cache.LoadingCache;public class DuplicationEliminator &#123; /** * 每一部分占用的位数 */ private final static long SEQUENCE_BIT = 12; // 序列号占用的位数 private final static long MACHINE_BIT = 8; // 机器标识占用的位数 private final static long MILLI_BIT = 10; // 毫秒占用的位数 /** * 每一部分向左的位移 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long TIMESTMP_LEFT = MACHINE_LEFT + MACHINE_BIT; /** * 起始的时间戳2014/01/01，可以在30年之内保证id的位数维持在18位 */ private final static long START_STMP = 1388505600000L; private static final int RETENTION_MINUTES = 1; private LoadingCache&lt;Long, BitSet&gt; cache = CacheBuilder.newBuilder()// .refreshAfterWrite(RETENTION_MINUTES, TimeUnit.MINUTES)// 给定时间内没有被读/写访问，则回收。 .build(new CacheLoader&lt;Long, BitSet&gt;() &#123; private final static int MAX_MILLI = (int) (-1L ^ (-1L &lt;&lt; MILLI_BIT)); @Override public BitSet load(Long key) throws ExecutionException &#123; // BitSet的值由sequence和毫秒数组合而成,每秒并发不超过1000的topic一般sequence都是0,所以这里设置初始size是MAX_MILLI,避免过多的扩容开销 return new BitSet(MAX_MILLI); &#125; &#125;); public boolean putIfAbsent(long id) throws ExecutionException &#123; Pair&lt;Long, Integer&gt; pair = idToPair(id); long key = pair.getKey(); int value = pair.getValue(); BitSet existingValues = cache.get(key); if (existingValues.get(value)) &#123; return false; &#125; else &#123; existingValues.set(value); return true; &#125; &#125; /** * 将id转成key-value pair，便于cache存储 * * @param id * @return */ private Pair&lt;Long, Integer&gt; idToPair(long id) &#123; int seq = (int) ((id) &amp; ~(-1L &lt;&lt; SEQUENCE_BIT)); long machineId = (id &gt;&gt; MACHINE_LEFT) &amp; ~(-1L &lt;&lt; MACHINE_BIT); long timestamp = (id &gt;&gt; TIMESTMP_LEFT) + START_STMP; long sec = timestamp / 1000; int milli = (int) (timestamp % 1000); return Pair.of(sec &lt;&lt; MACHINE_BIT | machineId, seq &lt;&lt; MILLI_BIT | milli); &#125; /** * @param pair * @return */ private long pairToId(Pair&lt;Long, Integer&gt; pair) &#123; long key = pair.getKey(); int value = pair.getValue(); long sequence = (value) &gt;&gt; MILLI_BIT; long machineId = (key) &amp; ~(-1L &lt;&lt; MACHINE_BIT); long timestamp = (key &gt;&gt; MACHINE_BIT) * 1000 + (value &amp; ~(-1L &lt;&lt; MILLI_BIT)); return (timestamp - START_STMP) &lt;&lt; TIMESTMP_LEFT // 时间戳部分 | machineId &lt;&lt; MACHINE_LEFT // 机器标识部分 | sequence; // 序列号部分 &#125;&#125; 测试 测试写入一亿个id，耗时48.4s，QPS=206.6万，内存占用68MB，性能和开销都还可以 1234567891011public static void main(String[] args) throws ExecutionException, InterruptedException &#123; DuplicationEliminator eliminator = new DuplicationEliminator(); Stopwatch watch = Stopwatch.createStarted(); for (int i = 0; i &lt; 1_0000_0000; i++) &#123; long id = IdGenerator.generateId(); if (!eliminator.putIfAbsent(id)) &#123; System.out.println("duplicated: " + id); &#125; &#125; System.out.println(watch.elapsed(TimeUnit.MILLISECONDS));&#125;]]></content>
      <tags>
        <tag>snowflake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto在Windows环境下编译错误]]></title>
    <url>%2F2019%2F01%2F07%2F2019-01-07_Presto%E5%9C%A8Windows%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%BC%96%E8%AF%91%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[最近在尝试重写presto-kafka，对接kafka1.0，在Windows环境下编译打包presto-kafka时候报错 123[ERROR] Failed to execute goal io.takari.maven.plugins:presto-maven-plugin:0.1.12:generate-service-descriptor (default-generate-service-descriptor) on project presto-jmx: Execution default-generate-service-descriptor of goal io.takari.maven.plugins:presto-maven-plugin:0.1.12:generate-service-descriptor failed: A required class was missing while executing io.takari.maven.plugins:presto-maven-plugin:0.1.12:generate-service-descriptor: com\facebook\presto\kafka\KafkaColumnHandle (wrong name: com/facebook/presto/kafka/KafkaColumnHandle) Presto Github上说的很明确，Presto只支持在Linux环境下编译 官方在Issues里也确认了这一点，但从报错信息上来看，似乎是类名不对，把presto-maven-plugin的代码拉下来看了一下 123line 122String className = classPath.substring(0, classPath.length() - 6).replace('/', '.'); 类名是通过把类路径的文件分隔符替换成点号来生成，但问题是Windows的分隔符是反斜杠\，而不是/，所以在Windows环境下这个替换没起作用，造成ClassNotFoundException，把&#39;/&#39;替换成File.separatorChar就好了 另外还碰到的一个问题是 1[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (checkstyle) on project presto-kafka: You have 43401 Checkstyle violations. -&gt; [Help 1] 原因是重写之后的代码不符合规范，maven-checkstyle-plugin检查失败，如果不打算根据presto的规范改的话，一个粗暴的解决办法是在pom.xml添加properties：&lt;air.check.skip-extended&gt;true&lt;/air.check.skip-extended&gt;，直接忽略检查 还有一个类似的问题是jar包冲突的检查，可以通过&lt;air.check.fail-dependency&gt;false&lt;/air.check.fail-dependency&gt;忽略]]></content>
      <tags>
        <tag>presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Schema Registry报错50001]]></title>
    <url>%2F2018%2F12%2F10%2F2018-12-10_Schema%20Registry%E6%8A%A5%E9%94%9950001%2F</url>
    <content type="text"><![CDATA[{“error_code”:50001,”message”:”Register schema operation failed while writing to the Kafka store”} 今天被这个异常搞得很头大，创建/更新/删除schema的时候都报50001错误，官方文档上是这么写的Error code 50001 – Error in the backend datastore，应该是访问后端存储，也就是kafka的_schema这个topic失败，具体失败的原因不清楚，所以看看schema registry的日志，然后。。日志里没有错误。。。再看下kafka日志。。也没有错误。。。 搜了一下，github有人提了个issue说是message size过大，调整producer的max.request.size和topic level的message.max.bytes和replica.fetch.max.bytes就好了，但理论上我们应该没有哪个schema会超过1M，一般都不会超过100个字段，但还是调整参数试了一下，没用。。。 没办法只能看源码了，然后发现源码里不！打！印！错！误！日！志！只是封装了Exception的基本信息（Error in the backend datastore。。。）和Error Code返回给客户端，然后详细错误信息。。吞了。。。牛X。。 给exception加上log，重新编译打包重启，问题其实很简单，producer超时了。schema registry默认producer timeout是500ms，可以通过kafkastore.timeout.ms参数修改，调成2000ms问题就解决了。之所以今天会超时，是有一台broker今天下掉做磁盘raid，重新上线之后数据都清空了，需要从别的broker恢复replica，说来惭愧，这台机器还没来得及升万兆网卡，所以流量几乎打满带宽，而_schema这个topic的其中一个备份刚好在这台机器上，并且schema registry producer的acks默认是-1，即要求所有备份完成之后才会返回response，在流量紧张的情况下500ms之内没有完成写入和备份操作，造成超时。这里顺带提一下，带宽不足的情况下，一定注意server properties的num.recovery.threads.per.data.dir的设置，一般千兆带宽的话默认值1已经就是极限了，经过测试设置为2就会压满带宽，数据正常写入和正常备份都会受影响 就这么个小事情浪费了一下午的时间，如果有日志的话应该几分钟就能解决，所以打算把schema registry的日志完善一下，顺便接入日志告警平台，一劳永逸]]></content>
      <tags>
        <tag>kafka</tag>
        <tag>schemaregistry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume追踪数据来源]]></title>
    <url>%2F2018%2F10%2F20%2F2018-10-20_Flume%E8%BF%BD%E8%B8%AA%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90%2F</url>
    <content type="text"><![CDATA[需求 我司的实时平台是用Flume来做实时数据的采集,多数场景下是用户发送TCP请求,Flume通过MultiportSyslogTCPSource接收 流平台的一个关键的监控环节是数据来源，但通过Flume采集下来数据并不带有数据来源的信息，这样就给整个数据流的监控带来很大的麻烦，比如收到某些不合规范的数据，导致Avro转换失败，如果能定位到某个表的话还不算太糟糕，至少还能联系到owner，更坏的情况是收到一些垃圾信息，根本定位不到属于哪个应用或者表，冤无头债无主，一脸懵逼 解决方案1.0 改造的思路是，在MultiportSyslogTCPSource中接收数据的同时，将client端的ip写到flume event header中，再把header的信息通过kafka sink带到kafka message的headers中，这样后续流处理发生异常时就可以把client ip打印出来，便于找人背锅 代码也很简单，在MultiportSyslogTCPSource.java的内部静态类MultiportSyslogHandler的messageReceived函数中，从session中取出RemoteAddress，设置到header中 12345678910if (lineSplitter.parseLine(buf, savedBuf, parsedLine)) &#123; Event event = parseEvent(parsedLine, decoder); if (portHeader != null) &#123; event.getHeaders().put(portHeader, String.valueOf(port)); &#125; event.getHeaders().put(REMOTE_ADDRESS, ((InetSocketAddress) session.getRemoteAddress()).getAddress().getHostAddress()); events.add(event);&#125; else &#123; logger.trace("Parsed null event");&#125; 在KafkaSink.java的process函数中，把flume header的所有entry（或者可以只添加key为REMOTE_ADDRESS的entry）添加到kafka record的headers中 1234Map&lt;String, String&gt; headers = event.getHeaders();for(Entry&lt;String, String&gt; entry: headers.entrySet()) &#123; record.headers().add(entry.getKey(), entry.getValue().getBytes());&#125; 解决方案2.0 问题还没完，由于我们使用nginx来做的HA和负载均衡，后来发现从header里取出来的client ip都是nginx的ip而不是真正客户端的ip。这是因为nginx做反向代理会把request和response转一手，屏蔽掉client和server的信息，如果直接修改request，把nginx的ip改为真正client的ip，server端就会把response发送到真正的client，这样很可能会由于网络不通或者被防火墙拦截掉而响应失败 这个问题在nginx本身是无解的，所以考虑引入某种机制，在请求体中携带client ip，对比了几种方案之后最终确定用代理协议Proxy Protocol Nginx开启Proxy Protocol只需要添加配置proxy_protocol on;，例如 1234567server &#123; listen 6240; proxy_connect_timeout 3s; proxy_timeout 120s; proxy_pass tcp_5240; proxy_protocol on;&#125; 开启之后，每条数据都会添加一个header（实际上就是在发送原数据之前先发送一条proxy protocol信息），proxy protocol的格式如下 1PROXY 协议栈 源IP 目的IP 源端口 目的端口 例如： 1PROXY TCP4 10.0.0.2 192.168.0.1 49863 6240 这样flume只需要从header中解析出源IP然后event header中就可以了，具体做法如下（在MultiportSyslogTCPSource基础上改造）： 123456789String msg = parseMsg(parsedLine, decoder);if (StringUtils.isNotEmpty(msg) &amp;&amp; !ProxyProtocolUtil.acceptProxyMetadata(session, msg)) &#123; Event event = parseEvent(parsedLine, decoder, msg); if (portHeader != null) &#123; event.getHeaders().put(portHeader, String.valueOf(port)); &#125; event.getHeaders().put(REMOTE_ADDRESS, ProxyProtocolUtil.getRemoteAddress(session)); events.add(event);&#125; 从ParsedBuffer中构造一行数据，然后通过ProxyProtocolUtil.acceptProxyMetadata判断是否符合proxy protocol格式，如果符合，则将源IP提取出来并放到session中；如果不符合，则说明这是原数据，用来构造event，并把session中的源IP写入header中。后续处理和解决方案1.0中一样。ProxyProtocolUtil.acceptProxyMetadata的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class ProxyProtocolUtil &#123; private static final String CLIENT_ADDRESS = "ClientAddress"; private static final Logger LOGGER = LoggerFactory.getLogger(ProxyProtocolUtil.class); // 108 bytes is the largest buffer needed for the PROXY protocol, but we are a // bit more lenient public static final int MAX_PROXY_HEADER_LENGTH = Byte.MAX_VALUE; public static final String PROX_PROTOCOL_PREFIX = "PROXY"; public static boolean acceptProxyMetadata(IoSession session, String msg) &#123; if (!msg.startsWith(PROX_PROTOCOL_PREFIX)) &#123; if (LOGGER.isDebugEnabled()) &#123; LOGGER.debug("acceptServerProxyMetadata(session=&#123;&#125;) mismatched protocol header: expected=&#123;&#125;", new Object[] &#123; session, PROX_PROTOCOL_PREFIX &#125;); &#125; return false; &#125; try &#123; return parseProxyHeader(session, msg); &#125; catch (Exception e) &#123; LOGGER.debug("Not Proxy-Protocol. Msg = " + msg, e); return false; &#125; &#125; public static String getRemoteAddress(IoSession session) &#123; try &#123; if (session.containsAttribute(CLIENT_ADDRESS)) &#123; return ((InetSocketAddress) session.getAttribute(CLIENT_ADDRESS)).getAddress().getHostAddress(); &#125; else &#123; return ((InetSocketAddress) session.getRemoteAddress()).getAddress().getHostAddress(); &#125; &#125; catch (Exception e) &#123; return "UNKNOWN"; &#125; &#125; private static boolean parseProxyHeader(IoSession session, String msg) &#123; String[] proxyFields = GenericUtils.split(msg, ' '); // Trim all fields just in case more than one space used for (int index = 0; index &lt; proxyFields.length; index++) &#123; String f = proxyFields[index]; proxyFields[index] = GenericUtils.trimToEmpty(f); &#125; String proxyProtocolPrefix = proxyFields[0]; ValidateUtils.checkTrue(PROX_PROTOCOL_PREFIX.equalsIgnoreCase(proxyProtocolPrefix), "Mismatched protocol prefix: %s", proxyProtocolPrefix); String protocolVersion = proxyFields[1]; if ("TCP4".equalsIgnoreCase(protocolVersion) || "TCP6".equalsIgnoreCase(protocolVersion)) &#123; String layer3SrcAddress = proxyFields[2]; String layer3DstAddress = proxyFields[3]; String layer3SrcPort = proxyFields[4]; String layer3DstPort = proxyFields[5]; LOGGER.debug("parseProxyHeader(session=&#123;&#125;) using &#123;&#125;:&#123;&#125; -&gt; &#123;&#125;:&#123;&#125; proxy", new Object[] &#123; session, layer3SrcAddress, layer3SrcPort, layer3DstAddress, layer3DstPort &#125;); session.setAttribute(CLIENT_ADDRESS, new InetSocketAddress(layer3SrcAddress, Integer.parseInt(layer3SrcPort))); &#125; else &#123; LOGGER.warn("parseProxyHeader(session=&#123;&#125;) unsuppored sub-protocol - &#123;&#125; - continue as usual", session, protocolVersion); &#125; return true; &#125;&#125;]]></content>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume UDP Source丢包问题]]></title>
    <url>%2F2018%2F10%2F13%2F2018-10-13_Flume%20UDP%20Source%E4%B8%A2%E5%8C%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题描述 有用户提出通过udp协议发送数据到实时平台，所以考虑在flume接收节点添加udp source来接收udp请求 flume配置如下 1234a2.sources.syslog_udp_src.type=syslogudpa2.sources.syslog_udp_src.host=0.0.0.0a2.sources.syslog_udp_src.port=5340a2.sources.syslog_udp_src.channels=test_channel 系统环境如下 123CentOS Linux release 7.2.15114核cpu8G内存 在测试中发现当发送速度超过500/s时开始出现数据丢失，开始以为是网络丢包，但通过tcpdump抓包比对发现并没有丢包 netstat -su命令显示有数据包接收失败123456Udp: 4329216468 packets received 329761 packets to unknown port received. 301745872 packet receive errors 337569082 packets sent RcvbufErrors: 1061789 packets received表示application接收到的数据包的数量 packets to unknown port表示由于端口没有打开而丢失的数据包数据量，例如应用挂掉或者重启 packet receive errors表示接收数据包的异常的数量，需要注意的是error和接收失败的数据包并不是一对一的关系，即有一个数据包有可能会产生多个error 问题原因 当客户端发送一个udp请求到flume时，整个流转顺序是这样的1Client -&gt; Kernel UDP socket buffer(4MB) -&gt; Flume UDP Source buffer(64KB) -&gt; Flume Channel buffer(磁盘，file channel) flume通过SyslogUDPSource接收UDP请求，底层是使用Netty的OioDatagramChannelFactory来创建服务器端channel，实现BIO（阻塞式IO）。BIO会对每个请求分配一个线程来处理，这种处理模式效率很低。经过测试，Flume UDP只能达到1100+的QPS，0.324MB/s 当client的发送速度大于flume udp source的接收速度时，数据开始在Kernel UDP socket buffer堆积，当buffer满了的时候，后续的请求就会被丢弃，发生packet receive errors 即使平均吞吐量没有超过flume udp的接收能力，但如果出现发送速度波动或者网络波动导致短时间内Kernel UDP socket buffer被填满，也会出现接收失败 解决方案 网上看了一下，有的建议扩大Kernel UDP socket buffer，比如50MB12sysctl -w net.core.rmem_max=52428800sysctl -w net.core.netdev_max_backlog=2000 但这只能在一定程度上避免波动的问题，如果发送速度大于接收速度，再大的缓存总会有满的时候 用NIO重写flume SyslogUDPSource，提高处理效率，目前并没有这么做的打算，因为采取了下一个方案 部署syslog-ng接收udp请求，并转发到flume tcp source。Syslog-ng是一个轻量框架，并不需要对数据进行过多的处理和缓存，可以直接对udp请求进行转发，并且使用的是多路复用的NIO，因此处理效率很高。即使是这样，经过压力测试发现也同样是有瓶颈，当qps大于24000时开始出现同样的packet receive errors，因为无论如何syslog-ng也是有处理上限的 122.5W QPS，丢包率0.008%10W QPS，丢包率2.2% 以上两种方案在数据量大到一定程度的时候都不可避免的丢包，所以如果可以的话，应该在数据发送之前就进行必要的过滤，把不需要的数据扔掉，降低请求数 最终的解决方案，也是我最推崇的： 放弃UDP，改用TCP]]></content>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es sql site server添加认证信息]]></title>
    <url>%2F2018%2F08%2F08%2F2018-08-08_es%20sql%20site%20server%E6%B7%BB%E5%8A%A0%E8%AE%A4%E8%AF%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[Elasticsearch的权限可以通过search-guard或者xpack(shield)来实现，Elasticsearch-SQL也同步支持了权限认证，只需要把用户名和密码作为参数添加到url中就可以了,例如：http://domain/?base_uri=http://&lt;es_addr&gt;/&amp;username=&lt;usr&gt;&amp;password=&lt;pwd&gt; 具体实现也不复杂，只涉及到前端controllers.js的改动 12345678910111213141516171819202122// settingsvar settings = location.search.substring(1).split("&amp;").reduce(function (r, p) &#123; r[decodeURIComponent(p.split("=")[0])] = decodeURIComponent(p.split("=")[1]); return r;&#125;, &#123;&#125;);var elasticsearchSqlApp = angular.module('elasticsearchSqlApp', ["ngAnimate", "ngSanitize"]);// authif (settings['username']) localStorage.setItem("auth", "Basic " + window.btoa(settings['username'] + ":" + settings['password']));if (localStorage.getItem("auth")) &#123; elasticsearchSqlApp.config(function ($httpProvider) &#123; $httpProvider.interceptors.push(function () &#123; return &#123; request: function (config) &#123; config.headers['Authorization'] = localStorage.getItem("auth"); return config; &#125; &#125;; &#125;); &#125;);&#125; 通过location.search读取url中的用户名和密码，然后用window.btoa转成base64字符串组装认证header 123headers: &#123; "Authorization": "Basic " + window.btoa(username + ":" + password)&#125; 认证信息通过修改$httpProvider对所有的$http生效，这样所有的es请求(/,/_nodes,/_sql,/_sql,/_explain等等)都会带上认证信息，然后在elasticsearch.yml中配置允许http请求就可以了 123http.cors.enabled: truehttp.cors.allow-origin: "*" 但并不能正常使用，界面显示异常 Error occured! response is not avalible. DEBUG信息显示 Request header field Authorization is not allowed by Access-Control-Allow-Headers in preflight response. 后端服务不允许在header中使用Authorization。解决方法是在elasticsearch.yml中添加配置 1http.cors.allow-headers: "Content-Type, Access-Control-Allow-Headers, Authorization, X-Requested-With, Content-Length" http.cors.allow-headers的默认值是X-Requested-With, Content-Type, Content-Length，修改之后就能通过认证并进行ES SQL查询了 另外还发现一个小问题，当访问了一个需要认证的ES集群之后，再访问另一个不需要认证的集群，就访问失败，原因是第一次访问之后前端缓存localStorage中就带有认证信息，第二次访问由于没有带用户名和密码参数，所以localStorage不会更新，导致第二次访问也带上了缓存的认证信息，导致访问失败。解决办法也比较简单，如果参数中没有用户名和密码，就把localStorage中的auth清空就行 12345if (settings['username']) &#123; localStorage.setItem("auth", "Basic " + window.btoa(settings['username'] + ":" + settings['password']));&#125; else &#123; localStorage.removeItem("auth");&#125;]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES浮点数被识别为字符串]]></title>
    <url>%2F2018%2F08%2F01%2F2018-08-01_ES%E6%B5%AE%E7%82%B9%E6%95%B0%E8%A2%AB%E8%AF%86%E5%88%AB%E4%B8%BA%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[最近准备把ES的版本从5.1.2升到6.2.4，将Kafka的数据写入ES的工具类ESPersistor需要进行相应api的调整。在5.1.2的java api中，使用IndexRequest.source(String source)来设置要写入的json字符串，但在6.2.4中这个函数已经被移除，可选的替代者有以下几种（source的重载函数还有很多，但这里不在讨论范围内） 12IndexRequest.source(String source, XContentType xContentType)IndexRequest.source(Map source, XContentType contentType) 第一种写法没有问题，指定XContentType.JSON就和之前版本的写入效果完全一样 第二种写法就发生了比较诡异的现象，假如json字符串中有值为浮点数，比如{“value”: 0.1}，写入ES之后类型并不是float，而是text。假如字段value之前并不存在，那么ES会自动创建类型为text的字段value，后续就没办法对value做数值类型的计算了。那么为什么浮点类型会被认为是字符串呢？看代码 123456789public IndexRequest source(Map source, XContentType contentType) throws ElasticsearchGenerationException &#123; try &#123; XContentBuilder builder = XContentFactory.contentBuilder(contentType); builder.map(source); return source(builder); &#125; catch (IOException e) &#123; throw new ElasticsearchGenerationException("Failed to generate [" + source + "]", e); &#125;&#125; 参数Map source实际上是会被转换成XContentBuilder来处理，再看builder.map(source); 1234567891011121314151617181920private XContentBuilder map(Map&lt;String, ?&gt; values, boolean ensureNoSelfReferences) throws IOException &#123; if (values == null) &#123; return nullValue(); &#125; // checks that the map does not contain references to itself because // iterating over map entries will cause a stackoverflow error if (ensureNoSelfReferences) &#123; CollectionUtils.ensureNoSelfReferences(values); &#125; startObject(); for (Map.Entry&lt;String, ?&gt; value : values.entrySet()) &#123; field(value.getKey()); // pass ensureNoSelfReferences=false as we already performed the check at a higher level unknownValue(value.getValue(), false); &#125; endObject(); return this;&#125; 先检查json(map)中是否有自我引用，然后遍历所有Entry，将key/value写到XContentBuilder中，再看看值是怎么写入的unknownValue(value.getValue(), false); 12345678910111213141516171819202122232425262728293031private void unknownValue(Object value, boolean ensureNoSelfReferences) throws IOException &#123; if (value == null) &#123; nullValue(); return; &#125; Writer writer = WRITERS.get(value.getClass()); if (writer != null) &#123; writer.write(this, value); &#125; else if (value instanceof Path) &#123; //Path implements Iterable&lt;Path&gt; and causes endless recursion and a StackOverFlow if treated as an Iterable here value((Path) value); &#125; else if (value instanceof Map) &#123; map((Map&lt;String,?&gt;) value, ensureNoSelfReferences); &#125; else if (value instanceof Iterable) &#123; value((Iterable&lt;?&gt;) value, ensureNoSelfReferences); &#125; else if (value instanceof Object[]) &#123; values((Object[]) value, ensureNoSelfReferences); &#125; else if (value instanceof Calendar) &#123; value((Calendar) value); &#125; else if (value instanceof ReadableInstant) &#123; value((ReadableInstant) value); &#125; else if (value instanceof BytesReference) &#123; value((BytesReference) value); &#125; else if (value instanceof ToXContent) &#123; value((ToXContent) value); &#125; else &#123; // This is a "value" object (like enum, DistanceUnit, etc) just toString() it // (yes, it can be misleading when toString a Java class, but really, jackson should be used in that case) value(Objects.toString(value)); &#125;&#125; 判断value的类型，如果是ES标准数据类型，直接从WRITERS中获取相应的Writer写入，例如对于Float，调用(builder, value) -&gt; builder.value((Float) value)写入；对于其他类型，调用相应的value重载函数写入；如果列举的类型都不匹配，则当做字符串来处理 DEBUG一下，发现JSONObject {“value”: 0.1} 执行到这的时候，value.getClass()居然是BigDecimal，跟所有列举的类型都不匹配，所有就当字符串处理了，写入ES时就成了{“value”: “0.1”}，那么为什么值的类型会变成BigDecimal呢？测试一下 123JSONObject data = new JSONObject();data.put("value", 0.1);System.out.println(data.get("value").getClass()); 打印结果是class java.lang.Double，没有问题，这是new一个JSONObject的情况，再测试一下字符串parse成JSONObject的情况 12JSONObject data = JSON.parseObject("&#123;\"value\":0.1&#125;");System.out.println(data.get("value").getClass()); 打印结果是class java.math.BigDecimal，OK破案了，真凶是fastjson，它在parseObject时会把Float识别为BigDecimal，看一下源码，parseObject会调用parse(String text, int features)函数，features的值是常量JSON.DEFAULT_PARSER_FEATURE，这个常量是由一系列的Feature位或计算出来的 12345678910111213public static int DEFAULT_PARSER_FEATURE;static &#123; int features = 0; features |= Feature.AutoCloseSource.getMask(); features |= Feature.InternFieldNames.getMask(); features |= Feature.UseBigDecimal.getMask(); features |= Feature.AllowUnQuotedFieldNames.getMask(); features |= Feature.AllowSingleQuotes.getMask(); features |= Feature.AllowArbitraryCommas.getMask(); features |= Feature.SortFeidFastMatch.getMask(); features |= Feature.IgnoreNotMatch.getMask(); DEFAULT_PARSER_FEATURE = features;&#125; 其中有个Feature是UseBigDecimal，这个Feature会使得DefaultJSONParser中会把Float转成BigDecimal 1234case LITERAL_FLOAT: Object value = lexer.decimalValue(lexer.isEnabled(Feature.UseBigDecimal)); lexer.nextToken(); return value; 问题根源找到了，解决也就不难了，自定义一个features，把Feature.UseBigDecimal从DEFAULT_PARSER_FEATURE中用异或去掉，然后JSON.parse使用自定义的features就可以了 123int features = JSON.DEFAULT_PARSER_FEATURE ^ Feature.UseBigDecimal.getMask();JSONObject data = (JSONObject) JSON.parse(data.toJSONString(), features);System.out.println(data.get("value").getClass()); 打印class java.lang.Double，解决]]></content>
      <tags>
        <tag>fastjson</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个坑掉两次。。]]></title>
    <url>%2F2018%2F05%2F14%2F2018-05-14_%E4%B8%80%E4%B8%AA%E5%9D%91%E6%8E%89%E4%B8%A4%E6%AC%A1%E3%80%82%E3%80%82%2F</url>
    <content type="text"><![CDATA[写一个ES的清理小工具，需要找出一个月之前创建的index if(createTS &lt; System.currentTimeMillis() - 30 24 3600 * 1000) 血崩。。。int越界。。。 然后找出超过100G的index if(size &gt; 100 1024 1024 * 1024) 再次血崩。。。 这种问题不应该，幸好不是PROD，要不就收拾细软了]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase不同版本集群之间数据迁移]]></title>
    <url>%2F2018%2F04%2F20%2F2018-04-20_HBase%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4%E4%B9%8B%E9%97%B4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[由于HBase CDH4和CDH5数据格式不兼容，所以不能用“CopyTable”之类的方法来进行数据迁移。取而代之的方法有两个： export + distcp + import export 在CDH4集群上，将制定表的数据导出为sequence file到指定目录，基本命令如下1hbase org.apache.hadoop.hbase.mapreduce.Export [options] &lt;tablename&gt; &lt;export_directory&gt; tablename： 需要导出的表名 export_directory： 数据导出到的hdfs目录 options：可以指定参数用于精细化的控制，格式为[-D &lt;property=value&gt;]*，例如123456789101112131415161718指定导出的sequence file压缩格式：-D mapreduce.output.fileoutputformat.compress=true-D mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec-D mapreduce.output.fileoutputformat.compress.type=BLOCK控制导出的内容：-D hbase.mapreduce.scan.column.family=&lt;familyName&gt;-D hbase.mapreduce.include.deleted.rows=true-D hbase.mapreduce.scan.row.start=&lt;ROWSTART&gt;-D hbase.mapreduce.scan.row.stop=&lt;ROWSTOP&gt;控制导出性能：-D hbase.client.scanner.caching=100-D mapreduce.map.speculative=false-D mapreduce.reduce.speculative=false对于大宽表，建议设置batch size：-D hbase.export.scanner.batch=10 distcp 将CDH4集群导出到export_directory目录中的sequence file拷贝到CDH5集群，这里用到hadoop的distcp命令，用于在不同hadoop集群间拷贝文件 1hadoop distcp -p -update -skipcrccheck hftp://cdh4-namenode:port/export_directory hdfs://cdh5-namenode/import_directory 注意distcp命令一定要在目标集群(CDH5)上执行 distcp会在文件拷贝完成后比较源文件和目标文件的checksum，由于CDH4和CDH5的默认checksum算法不一致，CDH4使用CRC32，CDH5使用CRC，因此任务有可能会失败，这里指定-skipcrccheck可以忽略这一步骤，或者通过-Ddfs.checksum.type=CRC32来指定checksum算法 import 在import之前，需要先在CDH5集群建表，column family必须和CDH4的表保持一致 然后将distcp过来的sequence file导入HBase表中，命令如下1hbase -Dhbase.import.version=0.94 org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;import_directory&gt; hbase.import.version指定源集群(CDH4)的HBase版本 拷贝HFile 另一种方案是直接将HFile从CDH4拷贝到CDH5的hdfs文件系统里，然后升级HFile distcp 1hadoop distcp -p -update -skipcrccheck webhdfs://cdh4-namenode:http-port/hbase hdfs://cdh5-namenode:rpc-port/hbase upgrade 启动CDH5集群，HBase会自动检测并升级HFile 总结总体来讲第一种方案耗时更长，因为需要进行三次mapreduce，但更建议用第一种方案，import/export的机制拥有更高的灵活性，你可以定时增量的迁移数据。除非数据量太大导致export和import耗费太长时间才考虑第二种方案。 ———————————- 我是分割线 ——————————————– 之前说到第二种方案做upgrade时需要重启集群，实际上有办法避免，方法如下 拷贝.tableinfo.0000000001文件 12hadoop fs -mkdir -p /hbase/data/default/&lt;tablename&gt;/.tabledeschadoop fs -mv /hbase/data/default/&lt;tablename&gt;/.tableinfo.0000000001 /hbase/data/default/&lt;tablename&gt;/.tabledesc 注：这一步是因为CDH4的.tableinfo.0000000001文件在根目录下，CDH5的在.tabledesc下 修复meta 1hbase hbck -fixMeta 重新分配rs 1hbase hbck -fixAssignments 完工]]></content>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SnowFlake算法在数据链路中的应用]]></title>
    <url>%2F2018%2F03%2F28%2F2018-03-28_SnowFlake%E7%AE%97%E6%B3%95%E5%9C%A8%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[数据链路追踪 在实时数据平台中,为了保证数据是可追溯的，一般会在数据生成时给每一条数据分配一个唯一的ID，以及生成的时间，最好再带上数据源的信息，这样DEBUG的时候就能知道数据是从哪来的。然后在数据的每个流转环节中，将每一条数据的ID和到达时间记录到日志或者其他存储中 这样在发生问题或者调试时，就能通过ID重现数据的轨迹，包括数据行经的每个组件以及消耗的时间。这个过程能够通过ELK轻松完成，将所有组件的日志导入ES，然后使用ID在Kibana中搜索，甚至可以通过定制图表来实现数据链路可视化，这里不做展开 以Flume为例，我们可以在最外层的Agent（即跟数据源最近的一层，了解Flume分层架构请自行搜索）通过UUIDInterceptor生成唯一ID，然后连同数据源信息（机房ID,机器ID）和数据接收的时间戳一起放到event header中，最后带到数据流里 注： 一般情况下数据的唯一ID和生成时间event_time应该由数据的生成方指定，如果没有，则由Flume生成。当然也可以强制由Flume生成，主要取决于是否将平台与用户之间的网络视同内网 这里有两个问题： 1.一般来讲实时平台的组件数据结构中都有一个rowkey来保存唯一ID，比如kafka message key，es index id，hbase rowkey等等，但不一定有header或者metamap之类的数据结构来保存数据源信息和数据接收时间，因此，如果能把这些额外的信息都保存到ID中就再好不过了 2.UUID太长了，36个字符，并且是字母数字和符号的组合，这非常影响存储的效率 所以可以考虑用SnowFlake算法来解决这两个问题 SnowFlake算法 先贴代码，来自Twitter雪花算法SnowFlake的Java实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class SnowFlake &#123; /** * 起始的时间戳 */ private final static long START_STMP = 1514736000000L; /** * 每一部分占用的位数 */ private final static long SEQUENCE_BIT = 12; // 序列号占用的位数 private final static long MACHINE_BIT = 5; // 机器标识占用的位数 private final static long DATACENTER_BIT = 5;// 数据中心占用的位数 /** * 每一部分的最大值 */ private final static long MAX_DATACENTER_NUM = -1L ^ (-1L &lt;&lt; DATACENTER_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L &lt;&lt; MACHINE_BIT); private final static long MAX_SEQUENCE = -1L ^ (-1L &lt;&lt; SEQUENCE_BIT); /** * 每一部分向左的位移 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long DATACENTER_LEFT = MACHINE_LEFT + MACHINE_BIT; private final static long TIMESTMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT; private long datacenterId; // 数据中心 private long machineId; // 机器标识 private long sequence = 0L; // 序列号 private long lastStmp = -1L;// 上一次时间戳 public SnowFlake(long datacenterId, long machineId) &#123; if (datacenterId &gt; MAX_DATACENTER_NUM || datacenterId &lt; 0) &#123; throw new IllegalArgumentException("datacenterId can't be greater than MAX_DATACENTER_NUM or less than 0"); &#125; if (machineId &gt; MAX_MACHINE_NUM || machineId &lt; 0) &#123; throw new IllegalArgumentException("machineId can't be greater than MAX_MACHINE_NUM or less than 0"); &#125; this.datacenterId = datacenterId; this.machineId = machineId; &#125; /** * 产生下一个ID * * @return */ public synchronized long nextId() &#123; long currStmp = getNewstmp(); if (currStmp &lt; lastStmp) &#123; throw new RuntimeException("Clock moved backwards. Refusing to generate id"); &#125; if (currStmp == lastStmp) &#123; // 相同毫秒内，序列号自增 sequence = (sequence + 1) &amp; MAX_SEQUENCE; // 同一毫秒的序列数已经达到最大 if (sequence == 0L) &#123; currStmp = getNextMill(); &#125; &#125; else &#123; // 不同毫秒内，序列号置为0 sequence = 0L; &#125; lastStmp = currStmp; return (currStmp - START_STMP) &lt;&lt; TIMESTMP_LEFT // 时间戳部分 | datacenterId &lt;&lt; DATACENTER_LEFT // 数据中心部分 | machineId &lt;&lt; MACHINE_LEFT // 机器标识部分 | sequence; // 序列号部分 &#125; private long getNextMill() &#123; long mill = getNewstmp(); while (mill &lt;= lastStmp) &#123; mill = getNewstmp(); &#125; return mill; &#125; private long getNewstmp() &#123; return System.currentTimeMillis(); &#125; public static void main(String[] args) &#123; SnowFlake snowFlake = new SnowFlake(2, 4); System.out.println(snowFlake.nextId()); &#125;&#125; SnowFlake算法用来生成64位的ID，刚好可以用long整型存储，能够用于分布式系统中生产唯一的ID， 并且生成的ID有大致的顺序。以上代码生成的64位ID可以分成5个部分： 0 - 41位时间戳 - 5位数据中心标识 - 5位机器标识 - 12位序列号 41位时间戳可以存储大概69年的时间，所以以1970/01/01为基准的话，能保存到2039年，而如果以START_STMP（2018/01/01）为基准存储（currentTimeMillis - START_STMP）,就能够延长到2087年 12位序列号，时间戳相同的记录通过使用递增的序列号来避免冲突，12位序列号能支持每毫秒4096个ID（如果一毫秒内请求书超过4096个，则超过的请求阻塞到下一毫秒，并发量会受到影响），即409.6万QPS，一般能够满足需求，如果不够的话，可以通过减少数据中心或机器的存储位数来增加序列化存储 10位数据源标识，这里拆分为5位数据中心标识和5位机器标识，其实只是逻辑上的区分，总共可支持标记1024个数据源 获得数据的ID之后，可以通过以下代码解析出数据的接收时间以及数据源信息 1234System.out.println("SEQUENCE = " + ((id) &amp; ~(-1L &lt;&lt; SEQUENCE_BIT)));System.out.println("MACHINE = " + ((id &gt;&gt; MACHINE_LEFT) &amp; ~(-1L &lt;&lt; MACHINE_BIT)));System.out.println("DATACENTER = " + ((id &gt;&gt; DATACENTER_LEFT) &amp; ~(-1L &lt;&lt; DATACENTER_BIT)));System.out.println("TIMESTAMP = " + ((id &gt;&gt; TIMESTMP_LEFT) + START_STMP)); Flume SnowFlake在Flume中的使用就很简单了，使用SnowFlakeIntercepter替换掉UUIDInterceptor即可，代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class SnowFlakeIntercepter implements Interceptor &#123; private String headerName; private boolean preserveExisting; private SnowFlake snowFlake; private boolean reverse; public static final String HEADER_NAME = "headerName"; public static final String PRESERVE_EXISTING_NAME = "preserveExisting"; public static final String DATACENTER_ID = "datacenter"; public static final String MACHINE_ID = "machine"; public static final String REVERSE = "reverse"; protected SnowFlakeIntercepter(Context context) &#123; reverse = context.getBoolean(REVERSE, true); headerName = context.getString(HEADER_NAME, "id"); preserveExisting = context.getBoolean(PRESERVE_EXISTING_NAME, true); snowFlake = new SnowFlake(context.getLong(DATACENTER_ID), context.getLong(MACHINE_ID)); &#125; @Override public void initialize() &#123; &#125; @Override public Event intercept(Event event) &#123; Map&lt;String, String&gt; headers = event.getHeaders(); if (preserveExisting &amp;&amp; headers.containsKey(headerName)) &#123; // preserve the existing id &#125; else &#123; String id = snowFlake.nextId() + ""; headers.put(headerName, reverse ? StringUtils.reverse(id) : id); &#125; return event; &#125; @Override public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123; List&lt;Event&gt; results = new ArrayList&lt;Event&gt;(events.size()); for (Event event : events) &#123; event = intercept(event); if (event != null) &#123; results.add(event); &#125; &#125; return results; &#125; @Override public void close() &#123; &#125; public static class Builder implements Interceptor.Builder &#123; private Context context; public Builder() &#123; &#125; @Override public SnowFlakeIntercepter build() &#123; return new SnowFlakeIntercepter(context); &#125; @Override public void configure(Context context) &#123; this.context = context; &#125; &#125;&#125; flume-conf.properties中配置的时候需要指定数据中心标识datacenter和机器标识machine，也可以通过指定reverse=false来关闭ID翻转，默认是true，因为SnowFlake生成的ID默认是递增的，虽然存储效率较高，但应用到某些存储系统时会产生热点问题，比如HBase 12345a2.sources.test_src.interceptors=i1a2.sources.test_src.interceptors.i1.type=com.sdo.dw.rtc.flume.interceptor.SnowFlakeIntercepter$Buildera2.sources.test_src.interceptors.i1.datacenter=2a2.sources.test_src.interceptors.i1.machine=4a2.sources.test_src.interceptors.i1.reverse=false 这里还有个坑，假如使用了ID翻转，并且在数据流中将ID当做数值类型处理，有可能会产生ID冲突，比如12345和123450在翻转并转成Long型之后都是54321。解决的办法也很简单，将START_STMP设置的足够小，比如0L，以保证生成的ID都足够大就没问题了 ———————————- 8/14 更新 ——————————————– 之前说到把START_STMP设置的足够小，比如0L，来避免翻转之后产生的冲突。但这里也有问题，SnowFlake算法是能保证产生的id是长整型，但19位的长整型翻转之后就有可能超出Long.MAX_VALUE，比如6433192668775325729，这会给后续的存储带来很大的麻烦。目前我的想法是把生成的id限制在18位，具体做法是数据中心和机器标识一共使用8位存储，然后START_STMP设置为2014/01/01，可以在30年之内保证id的位数维持在18位]]></content>
      <tags>
        <tag>flume</tag>
        <tag>snowflake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka从0.10.1升级到1.0.0]]></title>
    <url>%2F2018%2F01%2F30%2F2018-01-30_Kafka%E4%BB%8E0.10.1%E5%8D%87%E7%BA%A7%E5%88%B01.0.0%2F</url>
    <content type="text"><![CDATA[最近要把Kafka集群从0.10.1.1升级到1.0.0，主要是为了使用新的ProducerRecord和ConsumerRecord中的Header来保存和传输一些metadata，要求升级过程中不能服务不能中断，所以不考虑整体停机升级的方案，升级过程如下: 安装Kafka1.0，安装方法略，注意这里只是安装新版本Kafka，但并不启动，旧版本的Kafka保持正常工作 更新Kafka 1.0的server.properties，添加以下配置 12inter.broker.protocol.version=0.10.1log.message.format.version=0.10.1 inter.broker.protocol.version用于指定broker之间的通信协议版本，默认是和当前Kafka版本一致，即新版本中inter.broker.protocol.version为1.0。如果使用默认值，在轮询重启过程中，先重启的Broker将尝试使用1.0的协议与其他未重启的broker通信，由于未重启的broker仍旧是0.10.1版本，并不支持1.0的协议，会导致server.log中频繁报错，新版本broker上的partition无法被加回到ISR中1[2018-01-30 11:19:14,020] INFO [ReplicaFetcher replicaId=81, leaderId=83, fetcherId=0] Retrying leaderEpoch request for partition test_kane-2 as the leader reported an error: UNKNOWN_SERVER_ERROR (kafka.server.ReplicaFetcherThread) log.message.format.version用于指定broker持久化数据的格式，默认是和当前Kafka版本一致，即新版本log.message.format.version为1.0 轮询重启集群：关闭旧版本broker，再启动新版本broker 再次更新server.properties，并轮询重启集群 12inter.broker.protocol.version=1.0log.message.format.version=1.0 这里由于我需要使用Record的Header，因此直接把log.message.format.version改成了1.0，否则会报错java.lang.IllegalArgumentException: Magic v1 does not support record headers。但这样做会有风险，即如果producer的版本为1.0，consumer的版本为0.10.1，则consumer会因为无法解析1.0格式的数据而报错。所以建议确保所有consumer都升级到1.0之后，再把log.message.format.version改成1.0]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase1.2 java client使用心得]]></title>
    <url>%2F2017%2F12%2F15%2F2017-12-15_HBase1.2%20java%20client%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[最近对HBase做了升级，从0.92升级到了1.2.0-cdh5.11.1，版本跨度很大，而且从社区版转向了Cloudera。之所以改用CDH，是因为大概两年前在前一个公司做HBase CDH4到CDH5的升级，由于一开始搭建集群的时候就没用Cloudera Manager，所以导致升级非常痛苦，因为两个版本之间底层文件的格式变了，所以为了稳定，找了另外一批机器装了CDH5，然后实现源数据的双写，再用export+import把CDH4的数据全量导过去，最后稳定并行一段时间之后把CDH4停掉。整个流程听着就很麻烦，开销也很大（相当于资源*2），所以这次选择了先把0.92平移到CDH4，然后用Cloudera Manager升级，整个过程非常平滑 升级之后（准确说是升级之前就需要准备）发现HBase client的API已经有了很大的变动，所以也花了一些时间来研究，以下是一些心得 有些旧的类和函数已经被废弃或将要被废弃，尽量使用新的API，比如： 使用ConnectionFactory管理Connection 用Table替代HTable，用Connection替代HConnection，Admin替代HBaseAdmin等等 使用工厂类而不是原生的构造函数来生成类 获取table改用参数TableName，而不是之前的String或者byte[] Table的使用 以前的使用方式1234Configuration conf = HBaseConfiguration.create();HTable t = new HTable(conf, "mytable");t.put(new Put(row).add(fam, qual, value));t.close(); 现在的使用方式123456Configuration conf = HBaseConfiguration.create();Connection conn = ConnectionFactory.createConnection(conf)Table t = conn.getTable(TableName.valueOf("mytable"));t.put(new Put(row).add(fam, qual, value));t.close();conn.close; 无论是Connection, Table, Admin还是Scanner，用完就关，由于这些接口或多或少的都在client端或server端(Scanner)hold住了一些资源，所以对于性能来讲，及时关掉是非常必要的。另外，由于这些接口都继承了Closeable，所以使用Java的try语法能否非常容易的close掉，而在之前的版本需要在try之外声明，然后在finally中关闭，还要加上null check 12345try (Connection connection = ConnectionFactory.createConnection(conf); Admin admin = connection.getAdmin(); Table table = connection.getTable(tableName);) &#123; table.get(new Get(...))&#125; Connection是线程安全且非常重的对象，因为它包含了zookeeper的连接，socket连接等等，所以最好在一个应用中只创建一个Connection供多线程使用，并且在整个程序退出之前才关闭。与Connection相反，Table，Admin等都是相对比较轻量的组件，并且非线程安全，所以可以针对每个请求创建一个单独的实例，请求处理完就关闭 使用BufferedMutator实现批量/流式Puts，它替代了之前版本的手动flush，提供了高性能的流式写入]]></content>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume数据截断问题]]></title>
    <url>%2F2017%2F12%2F08%2F2017-12-08_Flume%E6%95%B0%E6%8D%AE%E6%88%AA%E6%96%AD%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题发送到SyslogUDPSource的单个请求数据大小超过2k的话会被截断，即只保留2k，其他数据丢失 原因SyslogUDPSource使用AdaptiveReceiveBufferSizePredictorFactory来处理buffer，理想的状况是buffer size会随着数据大小扩容，最大64k，但实际并没有。因为AdaptiveReceiveBufferSizePredictorFactory只针对NioDatagramWorker起作用，但SyslogUDPSource使用OioDatagramChannelFactory，buffer size永远是2k，超过就截断 解决办法改用FixedReceiveBufferSizePredictorFactory，固定大小64k，解决]]></content>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES Rollover Index]]></title>
    <url>%2F2017%2F11%2F01%2F2017-11-01_ES%20Rollover%20Index%2F</url>
    <content type="text"><![CDATA[Elasticsearch的TTL已经在5.0之后废除，它的原理是定时标记过期数据，然后从index中删除并执行merge，这个操作的开销是很大的，这就是TTL被废除的原因。作为替代，5.0之后加入了Rollover API，我们可以基于这个功能在某种程度上实现TTL语法 Rollover API的原理与之前一篇文章实时场景中Elasticsearch冷热索引分离中提到了index拆分是一样的，即当index的大小或者有效期（当前时间-创建时间）超过一定阈值时，滚动创建一个新的index Rollover API接受一个alias name和一个conditions列表作为参数。参数中的alias必须指向且只指向一个index。当index满足参数中指定的conditions中的任意一个时，ES会创建一个新的index并且将alias转而指向它，这里借用一张老图 基本语法 1234567891011121314151617# 创建一个名为logs-0000001的index，并赋予别名logs_writePUT /logs-000001 &#123; "aliases": &#123; "logs_write": &#123;&#125; &#125;&#125;# 当logs_write指向的index创建时间超过7天，或者documents大于等于1000个，或者包含的数据size超过5GB，则会创建一个新的index logs-0000002并将logs_write指向它，同时移除旧的index logs-0000001的别名POST /logs_write/_rollover &#123; "conditions": &#123; "max_age": "7d", "max_docs": 1000, "max_size": "5gb" &#125;&#125; Response如下： 12345678910111213&#123; "acknowledged": true, "shards_acknowledged": true, "old_index": "logs-000001", "new_index": "logs-000002", "rolled_over": true, "dry_run": false, "conditions": &#123; "[max_age: 7d]": false, "[max_docs: 1000]": true, "[max_size: 5gb]": false, &#125;&#125; rolled_over：index是否被滚动创建，即滚动条件是否满足dry_run：是否是测试演练，即不真正执行conditions：每个condition是否满足 命名新index 如果旧的index以-{number}结尾，比如logs-1，那么新的index将会沿袭旧的格式，并将结尾的number加1并自动填充0以保持数字长度为6，例如logs-000002，如果旧的index名字不以-{number}结尾，那么必须手动指定新的index名字 12345678POST /my_alias/_rollover/my_new_index_name&#123; "conditions": &#123; "max_age": "7d", "max_docs": 1000, "max_size": "5gb" &#125;&#125; 如果希望按日期来滚动生成并且命名index，那么可以使用ES提供的date math。Rollover API支持date math，但要求index必须以.{number}结尾，例如logstash-2016.02.03-1，无论日期是否相同，最后数字总会在滚动之后+1 123456789101112131415161718192021222324# 创建index并以当天日期命名，例如logs-2017.11.01-1PUT /%3Clogs-%7Bnow%2Fd%7D-1%3E &#123; "aliases": &#123; "logs_write": &#123;&#125; &#125;&#125;# 写入一条记录PUT logs_write/_doc/1&#123; "message": "a dummy log"&#125;# refresh以使该条记录立即生效POST logs_write/_refresh# 如果在当天执行以下请求，则滚动生成的新的index名称为logs-2017.11.01-000002，但如果是在第二天执行，则新的index名为logs-2017.11.02-000002POST /logs_write/_rollover &#123; "conditions": &#123; "max_docs": "1" &#125;&#125; 定义新index 我觉得按常理的话，滚动生成的新的index应该继承旧的index的mappings和settings，但实际上并不是，如果不指定的话，新的index会用ES默认的配置。当然你可以使用index templates或者在请求体中自定义配置，就和create index API一样 12345678910111213141516171819PUT /logs-000001&#123; "aliases": &#123; "logs_write": &#123;&#125; &#125;&#125;# 指定新的index的shards为2POST /logs_write/_rollover&#123; "conditions" : &#123; "max_age": "7d", "max_docs": 1000, "max_size": "5gb" &#125;, "settings": &#123; "index.number_of_shards": 2 &#125;&#125; 调试 Rollover API支持dry_run模式，即只是检查条件是否满足，但并不真正执行新建index和重定向alias的操作 123456789101112131415PUT /logs-000001&#123; "aliases": &#123; "logs_write": &#123;&#125; &#125;&#125;POST /logs_write/_rollover?dry_run&#123; "conditions" : &#123; "max_age": "7d", "max_docs": 1000, "max_size": "5gb" &#125;&#125;]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xms和xmx是否必须设置相等]]></title>
    <url>%2F2017%2F10%2F13%2F2017-10-13_xms%E5%92%8Cxmx%E6%98%AF%E5%90%A6%E5%BF%85%E9%A1%BB%E8%AE%BE%E7%BD%AE%E7%9B%B8%E7%AD%89%2F</url>
    <content type="text"><![CDATA[最近申请了几台机器专门用来运行Kafka Streams应用，发现经常发生OOM导致应用挂掉，原因是启动的时候指定了-Xms2G -Xmx2G，所以16G内存的机器没启几个应用内存就被占满了，尽管这些程序只是空跑。查了一下一般都建议-Xms和-Xmx设置为相等，那么能不能设置为不一样呢？ 先了解一下-Xms和-Xmx的含义 -Xms 初始堆的大小，也是堆大小的最小值，默认值是总共的物理内存/64（且小于1G），默认情况下，当堆中可用内存小于40%（这个值可以用-XX: MinHeapFreeRatio 调整，如-X:MinHeapFreeRatio=30）时，堆内存会开始增加，一直增加到-Xmx的大小 -Xmx堆的最大值，默认值是总共的物理内存1/4，如果Xms和Xmx都不设置，则两者大小会相同，默认情况下，当堆中可用内存大于70%（这个值可以用-XX: MaxHeapFreeRatio 调整，如-X:MaxHeapFreeRatio=60）时，堆内存会开始减少，一直减小到-Xms的大小可以通过以下命令查看系统默认的InitialHeapSize和MaxHeapSize：1java -XX:+PrintFlagsFinal -version | grep HeapSize-Xms=-Xmx？ 如果设定一个比较小的初始堆大小，并且程序需要的内存大于-Xms时，JVM将会分配更多的堆内存，且有可能需要移动一些objects和记账（book-keeping），这些动作都比较耗时，会使程序响应变慢，极端情况下甚至会导致OOM。假如程序在闲时和忙时的负载差别比较大，堆内存就会不停的伸缩。如果设置-Xms=-Xmx就不会有这样的问题 但是在测试环境或者开发环境中，Kafka并不会一直有数据（至少我的这个测试环境是这样，非UA），所以大部分时间Kafka Streams程序都是在空跑，所以堆内存会长期处于-Xms的状态。这种情况下设置-Xms=-Xmx是极大的浪费 结论 在生产环境，设置-Xms和-Xmx相等以保证程序稳定。在开发测试环境，设置一个比较小的-Xms，以支撑更多的测试程序运行]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Missing artifact jdk.tools：jdk.tools：jar：1.6]]></title>
    <url>%2F2017%2F09%2F30%2F2017-09-30_Missing%20artifact%20jdk.tools%2F</url>
    <content type="text"><![CDATA[Maven错误信息：Missing artifact jdk.tools:jdk.tools:jar:1.6 最近发现只要在Maven项目中引入HBase依赖，就会报错Missing artifact jdk.tools:jdk.tools:jar:1.6 在网上搜，基本都是说引入jdk.tools依赖就行，比如1234567&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt; 但这个办法治标不治本啊，为什么会jdk.tools找不到呢？比较诡异的是，同事用IDEA就没问题，所以我怀疑问题出在Eclipse上（我老人家还是习惯用Eclipse。。。） 经过排查发现启动Eclipse使用的JRE（默认在C:\Program Files下）没有tools.jar，这个依赖包是JDK里的JRE才有的，所以需要在Eclipse配置文件（ECLIPSE_HOME/eclipse.ini）里指定启动JRE12-vmC:\Program Files\Java\jdk1.8.0_151\jre\bin\server\jvm.dll 注意：-vm后面一定要换行，并且这两行配置一定要在-vmargs前面 然后Maven -&gt; Update Project（Alt+F5）就可以了]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时场景中Elasticsearch冷热索引分离]]></title>
    <url>%2F2017%2F09%2F24%2F2017-09-24_%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E4%B8%ADElasticsearch%E5%86%B7%E7%83%AD%E7%B4%A2%E5%BC%95%E5%88%86%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[如果你的ES集群的每台机器性能有比较大的差距，比如其中几台机器有SSD而其他机器用的普通磁盘，或者部分机器内存比较大，并且低配机器已经拖累了整个集群的读写性能，那么你需要考虑把写入频率和搜索频率比较高的热索引放到高性能的机器上，以获得更高的读写效率，同时把冷索引移到性能较差的机器上，以做到物尽其用 node分组首先我们需要根据所在机器性能的高低给node(这里专指data node)分组，这样我们在分离索引的时候才能知道热索引应该移到哪些高性能节点上去。这可以在启动时通过给每个节点设置属性来实现。例如我们有一台高性能机器，那么在这台机器上启动ES时我们可以通过以下方式声明它是“高性能的”1bin/elasticsearch -Enode.attr.performance=high 或者在elasticsearch.yml中配置1node.attr.performance=high 但是注意，这里的声明仅仅只是声明了一个属性键值对“performance=high”了，相当于给节点逻辑上做了分组，但并没有任何实际的作用，你甚至可以声明“a=b”，但为了可维护性考虑，最好还是起有意义的名称 类似的，我们在另外一台低性能的机器上启动ES时声明它是“低性能的”1bin/elasticsearch -Enode.attr.performance=low 冷热分离然后可以通过维护index的settingsindex.routing.allocation.*来控制该索引分配到指定的“组”中，即移动到该组相应的节点上。例如我们把index1移动到高性能的机器上，把index2移动到低性能的机器上123456789PUT index1/_settings&#123; "index.routing.allocation.include.performance": "high"&#125;PUT index2/_settings&#123; "index.routing.allocation.include.performance": "low"&#125; 这样就做到了冷热索引的分离，今后无论是集群伸缩还是增减备份数，该索引的所有shards（包括replica）都只会被分配在指定的组内，这里要注意，如果任一组内所有的节点都down掉，则绑定在该组节点的index都会处于unassigned状态 另一个要考虑的问题是如何定义冷热索引。个人建议是先按照QPS将所有index排序，先将大部分QPS较高的index甚至所有index放到高性能组里，然后从QPS最低的index开始一个个移到低性能组里，直到两组index的读写性能基本均衡 index拆分通常我们认为冷索引是指那些不再新增或更新的数据，但在实时场景中，index一般都会持续更新，那么所有的index都是热索引，就没法做分离了。这种情况下我们就需要先做index拆分，具体做法是每隔一定时间，比如每天新建一个index，归为热索引，旧的index一般就不会再有更新，归为冷索引 这里有一个问题，新建的index是不能与原来重名的，所以我们加个时间后缀用于区分，例如test_index_20170924。但这个做法会带来一个新的麻烦，index的名称每天都在变，所以每次都需要以日期作为后缀才能找到当前的热索引 用别名可以解决这个问题 我们给这个这个index起一个别名叫test_alias，指向当日的index，比如test_index_20170924，第二天创建了一个新的index: test_index_20170925，然后把test_alias跟test_index_20170924的关联解除，然后把test_alias指向test_index_20170925。这样我们如果要读写当前热索引，只需要访问test_alias就可以了，从ES用户的角度来看，感觉不到index做了拆分1234567POST /_aliases&#123; "actions" : [ &#123; "remove" : &#123; "index" : "test_index_20170924", "alias" : "test_alias" &#125; &#125;, &#123; "add": &#123; "index": "test_index_20170925", "alias": "test_alias" &#125; &#125; ]&#125; 注意这里remove和add两个动作一定要放在一个transaction里完成，否则如果先remove再add，可能会导致alias短暂不可用，如果先add再remove，可能会导致有短暂时间写入异常，因为alias同时指向两个index，ES无法决定应该写入哪一个 热索引读写的问题解决了，但还是有一些搜索的问题需要考虑，一个是全文搜索，用通配符是可以的，比如test_index_*，但如果刚好有其他index的前缀也是test_index_就麻烦了，可以通过新建一个别名指向所有的index来解决。还有就是幂等性无法保证的问题，如果你需要用id来保证幂等性，并且无法保证相同id的数据会在同一天写入，那么这个冷热索引分离的方案是行不通的，因为即使id在旧的index里存在，但在新的index里找不到，只能当做一条新数据插入]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch写入速度优化]]></title>
    <url>%2F2017%2F09%2F09%2F2017-09-09_Elasticsearch%E5%86%99%E5%85%A5%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[批量操作 批量写入比单条写入能够提供更好的性能。BulkProcessor可以通过三个阈值控制批量大小，分别是数据条数，数据size和时间间隔。通常来讲更大的批量会带来更好的性能，但也并不尽然，如果单个批次数据量过大，高并发的情况下会导致集群内存压力增大，官方建议是不要一次发送几十兆的数据，从我测试的结果来看，这个说法过于乐观，建议不超过5M 多线程单线程批量发送数据无法达到最大效率。使用多线程或者多进程能够更充分地利用集群资源，同时也能减少fsync的开销，因为数据的集中发送可以减少磁盘交互 但要注意响应码TOO_MANY_REQUESTS (429)，在Java Client中会返回EsRejectedExecutionException异常，这说明数据总发送速度已经超过了ES集群的处理能力，这时就需要发送端限流，比如加入时间补偿机制。或者也可以调大ES的bulk queue size，通常并不建议这么做，但某些版本的bulk queue size确实过于小，比如5.1的默认值是50,而5.5是200，所以如果你使用的是5.1版本，则可以在elasticsearch.yml加入以下配置thread_pool.bulk.queue_size: 200 增大refresh interval默认的index.refresh_interval是1秒，使用默认配置的index会每隔一秒钟创建一个新的segment，这会给ES带来比较大的merge压力，因此如果对延迟没有特别高的要求，可以适当调大index.refresh_interval，比如10秒 离线批量导入时关闭refresh和replica如果你需要一次性从外部系统导入数据到ES，你可以通过设置index.refresh_interval为-1来关掉refresh，设置index.number_of_replicas为0来去掉备份，这样可以大大提升数据的导入速度。refresh关掉能够极大的减少segments，也减少了merge，而去掉备份之后数据只需要写入一次。导入完成之后可以再把refresh和replica打开。值得一提的是，增加备份只是简单的数据传输，这个过程很快，但如果是向一个有备份的index写入数据，实际上在primary shard和replica上都需要进行索引，因此开销会大很多 但要注意，虽然这可以提升导入性能，但风险也提高了，因为导入过程中如果有节点挂掉，那么这个节点上的shards就不可用了，但如果是离线导入就还好，大不了重新导入就行 关闭swap稍微了解一下操作系统就知道swap对ES简直就是灾难，性能低还是其次，最重要的是会很容易把机器给爆掉，可以使用以下方法关闭swap12345671. 临时生效 swapoff -a2. 重启生效 vi /etc/sysctl.conf 添加或修改vm.swappiness = 03. 在elasticsearch.yml中配置 bootstrap.memory_lock: true 给pagecache预留memory官方说了，一半内存给ES，另外一半内存给lucene，也就是文件系统缓存。这很好理解，在缓存中的操作肯定比磁盘IO要快得多 使用自动分配的id如果你在index request中指定了id，那么ES会先去检查这个id是否存在，如果不存在，就新增一条记录，如果存在，则覆盖，检查id的动作是非常耗时的，特别是数据量特别大的情况下。如果index request中不指定id，ES会自动分配一个唯一的id，省去了id检查的过程，写入速度就得到提高 但如果你希望使用es的id来保证幂等性的话就没别的选择了，只能自己指定id 使用更好的硬件主要是磁盘，ES是重度依赖磁盘IO的软件 最好是SSD，这没什么好说的，就是快 然后是多块盘做raid0，由于数据是分散在多块盘上，所以如果一块盘坏掉可能会导致所有index不能用，但可以通过replica来规避风险 index buffer size如果你有重度写入需求，那么最好保证在一个节点上indices.memory.index_buffer_size / shards_count &gt; 512MB（超过这个值索引性能并不会有太明显提高）。indices.memory.index_buffer_size的默认值是10%，也就是说如果机器内存是64G，你把一半32G分配给了ES，那么buffer size为3.2G，能够支撑6.4个疯狂写入的shards]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统可用性评判标准]]></title>
    <url>%2F2017%2F08%2F21%2F2017-08-21_%E7%B3%BB%E7%BB%9F%E5%8F%AF%E7%94%A8%E6%80%A7%E8%AF%84%E5%88%A4%E6%A0%87%E5%87%86%2F</url>
    <content type="text"><![CDATA[评判标准故障转移故障转移就是在某一个应用服务器不能服务用户请求的时候，通过一定的技术实现用户请求转移到其他应用服务器上来进行业务逻辑处理。例如使用nginx实现多个无状态实例的负载均衡，当其中一个或多个实例失效时，仍能通过其他实例提供服务 冗余备份冗余备份就是针对某一个服务通过服务器集群或多机房部署来达到服务器冗余和相互备份的目的。例如Kafka和HDFS的备份机制 超时设置超时设置就是主调服务在调用被调服务的时候设置一个超时等待时间Timeout，主调服务发现超时后，主动进入超时处理流程。超时设置的好处在于当某个服务不可用时，不至于整个系统发生连锁反应 异步调用采用异步调用的方式调用被调服务，有利于将主调服务和被调服务进行解耦，同时提高系统的处理性能 服务分级和降级对于不同的服务可以采用不同的处理方案，出现故障时应该优先保证核心服务的运行。 监控告警服务一旦出现问题能够及时发现，通过自动化处理，或者人工介入处理，从而达到缩短系统的不可用时间，提高可用性。常见的监控指标有：CPU、带宽、内存使用率、网络连接状态，系统调用错误，成功率，PV，UV等 防雪崩机制对于设计的任何一个系统，都需要进行容量的预估和最大容量设置，当外部请求量超过最大容量时，应该启动防雪崩机制，以避免大量外部请求把服务压跨而不能对外提供服务 流量缓冲机制在服务内可以建立队列，当流量过大时，储存一定的用户请求到队列，当流量偏小时再拿出来快速处理，从而达到削峰填谷的作用。例如在大数据存储中Kafka作为缓冲队列的应用 自动化测试对于每一次代码的提交，都能通过自动测试程序对整个服务进行整体的回归测试，这样可以快速地避免代码修改引入新的问题，而导致服务不稳定 关键指标正常运行时间百分比 描述通俗叫法可用百分比年停机时间基本可用性2个999%87.6小时较高可用性3个999.9%8.8小时具有故障自动恢复能力的可用性4个999.99%53分钟极高可用性5个999.999%5分钟 容灾恢复能力RPO 灾难过程中的最大数据丢失量RTO 系统从灾难状态恢复到可运行状态所需的时间 容灾等级RTORPO第一级&lt;2天&lt;7天第二级&lt;24小时&lt;7天第三级&lt;12小时&lt;1天第四级&lt;2小时&lt;1天第五级&lt;30分钟&lt;30分钟第六级&lt;10分钟0]]></content>
      <tags>
        <tag>可用性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch环境配置]]></title>
    <url>%2F2017%2F08%2F04%2F2017-08-04_Elasticsearch%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[修改最大打开文件数，大于等于65536 临时生效 ulimit -n 65536 重启生效 vi /etc/security/limits.conf@search soft nofile 65536@search hard nofile 65536 修改最大线程数，大于等于2048 临时生效 ulimit -u 2048 重启生效 vi /etc/security/limits.conf@search soft nproc 2048@search hard nproc 2048 修改最大mmap数，大于等于262144 临时生效 sysctl -w vm.max_map_count=262144 重启生效 vi /etc/sysctl.confvm.max_map_count = 262144 允许search用户锁住内存 临时生效 ulimit -l unlimited 重启生效 vi /etc/security/limits.conf@search soft memlock unlimited@search hard memlock unlimited 禁用swap 临时生效 swapoff -a 重启生效 vi /etc/sysctl.confvm.swappiness = 0]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Streams State Store]]></title>
    <url>%2F2017%2F07%2F25%2F2017-07-25_Kafka%20Streams%20State%20Store%2F</url>
    <content type="text"><![CDATA[Kafka Streams默认使用RocksDB来存储状态 In-memory or persistent ? 状态可配置为保存在内存中或者持久化，RocksDB对于两者都适用，并且可以通过Stores factory API在两者间切换。StateStoreSupplier被创建之后，可以用于Kafka Streams DSL API（high level），也可以用于Processor API（low level） 持久化 为了保存状态，RocksDB会把state store的内容flush到StreamsConfig.STATE_DIR_CONFIG指定的磁盘上 1config.put(StreamsConfig.STATE_DIR_CONFIG, "my-state-store") Changelog 出于灾备和扩展性的考虑，state store的内容同样会默认保存changelog到kafka的topic中，topic名字为&lt;application_id&gt;-&lt;state_store_name&gt;-changelog，你可以通过enableLogging和disableLogging来开启或关闭该功能 如果task由于异常或宕机造成中断，在原来的机器或另一台机器重启之后，Kafka Streams能够优先通过重放changelog恢复state store的内容，而不是在新的task中创建新的state store，这是Kafka Streams状态安全性的保证 如果在task运行过程中删除changelog对应的topic，Kafka Streams会在下一个checkpoint再次写入changelog，但如果在下个checkpoint之前task中断，state store的数据将无法恢复]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时计算平台架构]]></title>
    <url>%2F2017%2F07%2F23%2F2017-07-23_%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Streams自动创建内部topic]]></title>
    <url>%2F2017%2F07%2F22%2F2017-07-22_Kafka%20Streams%E8%87%AA%E5%8A%A8%E5%88%9B%E5%BB%BA%E5%86%85%E9%83%A8topic%2F</url>
    <content type="text"><![CDATA[运行一段kafka streams程序的时候，报出以下异常 123456789101112Properties settings = new Properties();settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-first-streams-application");settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "10.128.74.83:9092");StreamsConfig config = new StreamsConfig(settings);TopologyBuilder builder = new TopologyBuilder();builder.addSource("SOURCE", "source_topic").addProcessor("PROCESS1", MyProcessor::new, "SOURCE") .addStateStore(Stores.create("COUNTS").withStringKeys().withIntegerValues().inMemory().build(), "PROCESS1") .addSink("SINK3", "sink_topic", "PROCESS1");KafkaStreams streams = new KafkaStreams(builder, config);streams.start(); 123456Exception in thread &quot;StreamThread-1&quot; org.apache.kafka.streams.errors.StreamsException: stream-thread [StreamThread-1] Failed to rebalance at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:410) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:242)Caused by: org.apache.kafka.streams.errors.StreamsException: task [0_0] Could not find partition info for topic: my-first-streams-application1-COUNTS1-changelog at org.apache.kafka.streams.processor.internals.ProcessorStateManager.register(ProcessorStateManager.java:174) …… 从异常信息上看是topic:my-first-streams-application1-COUNTS1-changelog不存在。这个topic是kafka streams的内部topic，用于保存state store的changelog。topic的命名格式是：&lt;application.id&gt;--，这些内部topic应该会在kafka streams应用执行的时候被自动创建，但为什么这里会报topic找不到呢？ 联想到用kafka-topics.sh是需要传入zookeeper地址的，因为topic的metadata会保存在zk中，那么kafka streams也应该在配置中指定zk地址才能创建topic，加上以下配置之后程序就能跑起来了 1settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "10.128.74.83:2181");]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink使用lambda表达式]]></title>
    <url>%2F2017%2F07%2F12%2F2017-07-12_Flink%E4%BD%BF%E7%94%A8lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Java8出来之后，lambda表达式由于简单易读，在流式计算中的使用开始变得普遍。 同样,Flink也支持lambda表达式，例如我们改写一下wordcount样例 12345678910111213DataSource&lt;String&gt; lines = env.fromElements( "Apache Flink is a community-driven open source framework for distributed big data analytics,", "like Hadoop and Spark. The core of Apache Flink is a distributed streaming dataflow engine written", ...);lines.flatMap(new FlatMapFunction&lt;String, Object&gt;() &#123; @Override public void flatMap(String line, Collector&lt; Object&gt; out) throws Exception &#123; for (String word : line.split("\\W+")) &#123; out.collect(new Tuple2&lt;&gt;(word, 1)); &#125; &#125;&#125;).groupBy(0).sum(1).print(); 这段代码很简单，先把每一行按空格拆分成若干单词，并将每个单词和数字1组成一个Tuple，然后把所有Tuple按照单词聚合，计算出每个单词的出现次数 尝试用lambda表达式来替换FlatMapFunction，代码如下 12345lines.flatMap((line, out) -&gt; &#123; for (String word : line.split("\\W+")) &#123; out.collect(new Tuple2&lt;&gt;(word, 1)); &#125;&#125;).groupBy(0).sum(1).print(); 但当运行这段代码时，会抛出如下异常：12345678910Caused by: org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of &apos;Collector&apos; are missing. It seems that your compiler has not stored them into the .class file. Currently, only the Eclipse JDT compiler preserves the type information necessary to use the lambdas feature type-safely. See the documentation for more information about how to compile jobs containing lambda expressions. at org.apache.flink.api.java.typeutils.TypeExtractor.validateLambdaGenericParameter(TypeExtractor.java:1653) at org.apache.flink.api.java.typeutils.TypeExtractor.validateLambdaGenericParameters(TypeExtractor.java:1639) at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:573) at org.apache.flink.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:188) at org.apache.flink.api.java.DataSet.flatMap(DataSet.java:266) at TestFlink.main(TestFlink.java:21) 这是因为Flink在用户自定义的函数中会使用泛型来创建serializer，当我们使用匿名函数时，类型信息会被保留。但Lambda表达式并不是匿名函数，所以javac编译的时候并不会把泛型保存到class文件里。 解决办法有两种: 第一种办法在异常中已经提示，使用Eclipse JDT编译器会保留对lambda表达式来说必要的类型信息。在Maven中使用Eclipse JDT编译器，只需要在把下面的插件加入到pom.xml中1234567891011121314151617&lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;compilerId&gt;jdt&lt;/compilerId&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.tycho&lt;/groupId&gt; &lt;artifactId&gt;tycho-compiler-jdt&lt;/artifactId&gt; &lt;version&gt;0.21.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt;&lt;/plugins&gt; 另一种办法是，使用Flink提供的returns方法来指定flatMap的返回类型，123456text.flatMap((line, out) -&gt; &#123; for (String word : line.split("\\W+")) &#123; out.collect(new Tuple2&lt;&gt;(word, 1)); &#125;&#125;).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class)).groupBy(0).sum(1) .print(); returns函数接收TypeInformation类型的参数，这里我们创建TupleTypeInfo来指定Tuple的参数类型。]]></content>
      <tags>
        <tag>flink</tag>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Failed to allocate memory within the configured max blocking time]]></title>
    <url>%2F2016%2F10%2F09%2F2016-10-09_Failed%20to%20allocate%20memory%20within%20the%20configured%20max%20blocking%20time%2F</url>
    <content type="text"><![CDATA[Kafka版本0.9.0.1，发送数据时报错1Failed to allocate memory within the configured max blocking time 原因很明显，如果producer端缓存（buffer.memory，默认32M）满了的话，在一定时间（max.block.ms，默认60s）内如果数据无法被放入缓存，则抛出该异常。 发生该异常之后，producer的性能急剧下降，TPS从10000+降低到50. 经过分析，原因是在尝试把数据放入缓存之前（Buffer.allocate），会在Condition队列waiters中插入一条新记录，12Condition moreMemory = this.lock.newCondition();this.waiters.addLast(moreMemory); 如果一定时间内没有能放到缓存中，则直接抛出异常，moreMemory并没有从waiters中移除。 而producer的缓存刷新的逻辑（RecordAccumulator.ready）如下：123boolean exhausted = this.free.queued() &gt; 0;……boolean sendable = full || expired || exhausted || closed || flushInProgress(); 其中free.queued() 的返回值就是waiters.size()，因此如果抛出本文开头所说的异常，那么之后每发一条数据，sendable都会为true，所以每发一条数据都会刷新一次缓存，相当于是同步发送，效率非常差。 好在在0.10中已经修复了这个bug，在抛出异常之前，先调用1this.waiters.remove(moreMemory); JIRA https://issues.apache.org/jira/browse/KAFKA-3651]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka收发oversize数据]]></title>
    <url>%2F2016%2F10%2F01%2F2016-10-01_Kafka%E6%94%B6%E5%8F%91oversize%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[Kafka整个消息管道的默认口径是1M，换句话说，默认Producer只能向Kafka发送大小不超过1M的消息，Kafka内部也只能处理大小不超过1M的消息，Consumer也只能消费大小不超过1M的消息。 如果发送2M（为了方便计算，以下1M=1000K）大小的数据，client会报异常 1Exception in thread &quot;main&quot; java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.RecordTooLargeException: The message is 2000037 bytes when serialized which is larger than the maximum request size you have configured with the max.request.size configuration. 根据提示，在Producer config中设置max.request.size为2M+（注意必须大于2M，因为消息本身的metadata也会占用空间，比如上文日志中，一条包含2M数据的消息的大小是2M+37 byte），但服务器返回异常 1Exception in thread &quot;main&quot; java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.RecordTooLargeException: The request included a message larger than the max message size the server will accept. 服务器无法接收这么大的消息。那么修改Kafka的server.properties，添加message.max.bytes=2M，然后重启Kafka集群。发送成功。 然后尝试消费消息，报错 1Exception in thread &quot;main&quot; org.apache.kafka.common.errors.RecordTooLargeException: There are some messages at [Partition=Offset]: &#123;test-1=6863354&#125; whose size is larger than the fetch size 1048576 and hence cannot be ever returned. Increase the fetch size on the client (using max.partition.fetch.bytes), or decrease the maximum message size the broker will allow (using message.max.bytes). 根据提示，在Consumer config中设置max.partition.fetch.bytes为2M。消费成功。 但还没完，这时发现Kafka的bytes_out异常的高，几乎逼近带宽极限，但这时并没有客户端在往Kafka里发送数据，查看Kafka日志，发现有异常 1[2016-10-25 03:15:08,361] ERROR [ReplicaFetcherThread-0-1], Replication is failing due to a message that is greater than replica.fetch.max.bytes for partition [test,1]. This generally occurs when the max.message.bytes has been overridden to exceed this value and a suitably large message has also been sent. To fix this problem increase replica.fetch.max.bytes in your broker config to be equal or larger than your settings for max.message.bytes, both at a broker and topic level. (kafka.server.ReplicaFetcherThread) 原因是修改message.max.bytes之后，Kafka broker已经能接收2M的数据，但replication fetcher尝试备份时失败了，因为replica.fetch.max.bytes控制了最大的备份消息size是1M。由于replication fetcher会无限的重试备份，因此bytes_out会急剧升高。把replica.fetch.max.bytes设置成2M，并重启broker之后，问题解决。 由于可以通过topic-level参数max.message.bytes来控制每个topic所能接收的最大消息size，所以为了避免重启，可以将replica.fetch.max.bytes设置为一个比较大的值（比如10M），以后如果有某个topic需要收发oversize的消息，只需要修改该topic的max.message.bytes就行，不需要重启集群。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch Plugins]]></title>
    <url>%2F2016%2F09%2F24%2F2016-09-24_Elasticsearch%20Plugins%2F</url>
    <content type="text"><![CDATA[安利几个ES的插件： sql123GITHUB地址：https://github.com/NLPchina/elasticsearch-sql/blob/master/README.md?utm_source=tuicool&amp;utm_medium=referral在线安装：./bin/plugin -u https://github.com/NLPchina/elasticsearch-sql/releases/download/&#123;version&#125;/elasticsearch-sql-&#123;version&#125;.zip --install sql访问地址：http://localhost:9200/_plugin/sql/ kopf123GITHUB地址：https://github.com/lmenezes/elasticsearch-kopf/在线安装：./bin/plugin install lmenezes/elasticsearch-kopf/&#123;branch|version&#125;访问地址：http://localhost:9200/_plugin/kopf/ HQ123GITHUB地址：https://github.com/royrusso/elasticsearch-HQ#version-compatibility在线安装：./bin/plugin install royrusso/elasticsearch-HQ/&#123;version&#125;访问地址：http://localhost:9200/_plugin/HQ/ head123GITHUB地址：https://github.com/mobz/elasticsearch-head在线安装：./bin/plugin -install mobz/elasticsearch-head/&#123;version&#125;访问地址：http://localhost:9200/_plugin/head/ 安装这几个插件需要注意一下版本，如果插件版本和ES版本不适配的话是不能正常工作的，版本对应关系请参照相应插件的GITHUB。 如果你对于ES的查询不是很熟悉，或者对ES复杂的查询语句感到厌烦，sql插件对你肯定是个福音。除了WEB-UI之外，你也可以直接将它用在Simple Query中：http://localhost:9200/_sql?sql=select * from indexName limit 10 另外3个插件kopf，head和HQ的功能都大同小异，都是ES的监控和一些常用操作的封装。个人感觉head更偏向于数据搜索，kopf界面比较专业，但HQ的功能感觉更全面强大一些，个人推荐HQ。]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ShutdownHook没有移除导致内存泄漏]]></title>
    <url>%2F2016%2F09%2F21%2F2016-09-21_ShutdownHook%E6%B2%A1%E6%9C%89%E7%A7%BB%E9%99%A4%E5%AF%BC%E8%87%B4%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%2F</url>
    <content type="text"><![CDATA[今天做kafka高并发测试的时候发生内存泄漏，直接原因是producer关闭之后内存没有得到释放。根本原因是： 为了确保线程producer的connection断开，在线程里加了ShutdownHook 1234567891011121314public static void addShutdownHook(final Closeable closeable, final Boolean isClosed) &#123; Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; if (!isClosed.booleanValue()) &#123; try &#123; closeable.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;);&#125; 在producer close之后，由于ShutdownHook里保存着producer的句柄（closeable），导致producer的内存没有办法被JVM回收，线程多了之后，导致OOM 解决办法是，在producer.close()之后，调用Runtime.getRuntime().removeShutdownHook(shutdownHook);]]></content>
      <tags>
        <tag>java</tag>
        <tag>OOM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cloudera Manager安装]]></title>
    <url>%2F2016%2F09%2F07%2F2016-09-07_Cloudera%20Manager%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[准备一台虚拟机作为cm-server 配置ssh免密码登录，在cm-server上执行脚本1234567891011121314CLUSTER_NODES=localhost#replace ',' with spaceCLUSTER_NODES=$&#123;CLUSTER_NODES//,/ &#125;FILE=~/.ssh/id_rsa.pubif [ ! -f "$&#123;FILE&#125;" ]; then ssh-keygen -t rsa -P ''fifor i in $CLUSTER_NODESdo scp ~/.ssh/id_rsa.pub root@$&#123;i&#125;:/tmp ssh root@$&#123;i&#125; "cat /tmp/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys"done 安装jdk 创建链接，mkdir /usr/java;ln -s ${JAVA_HOME} /usr/java/default 安装perl 下载perl-5.16.2.tar.gz到cm-server上 解压 cd perl-5.16.2 ./Configure -des -Dprefix=/usr/local/perl -Dusethreads -Uversiononly make make install 安装cloudera manager 下载cloudera-manager-centos7-cm5.8.0_x86_64.tar.gz到cm-server上 解压到/opt目录下，tar -zxvf cloudera-manager-centos7-cm5.8.0_x86_64.tar.gz –C /opt 修改/opt/cm-5.8.0/etc/cloudera-scm-agent/config.ini的server_host为cm-server的ip地址 创建cm用户 useradd –system –home=/opt/cm-5.8.0/run/cloudera-scm-server –no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm 修改/etc/hosts，配置所有cm节点的IP和hostname，包括cm-server和cm-agent 其他系统配置 关闭防火墙 service iptables stop （临时关闭） chkconfig iptables off （重启后永久生效） 关闭SELinux setenforce 0 （临时生效） 修改 /etc/selinux/config下的 SELINUX=disabled （重启后永久生效） 修改/etc/sysctl.conf，在最后一行加上：vm.swappiness=0 修改/etc/rc.local，在最后一行加上：echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag 重启cm-server 克隆虚拟机cm-server，到所有cm agent节点 在cm-server上安装mysql 创建并配置数据库 create database &lt;data_name&gt; DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 需要创建的数据库有:cm,hive,sentry,oozie,hue root授权访问以上所有的数据库: grant all on *.* to root@’%’; grant all privileges on *.* to root@’%’ identified by ‘admin’; 拷贝JDBC驱动mysql-connector-java-5.1.39-bin.jar到cdh: /opt/cm-5.8.0/share/cmf/lib 初始化cm数据库： sh /opt/cm-5.8.0/share/cmf/schema/scm_prepare_database.sh mysql cm 准备parcels，用于安装CDH，下载parcels到cm-server: /opt/cloudera/parcel-repo，注意把.sha1文件的后缀改为.sha，且最好用sha1sum命令确认文件没有在下载中损坏 拷贝JDBC驱动mysql-connector-java-5.1.39-bin.jar到/opt/cloudera/parcels/CDH/lib/hive/lib和/var/lib/oozie 启动cm-server，sh /opt/cm-5.8.0/etc/init.d/cloudera-scm-server start 启动每个cm-agent，sh /opt/cm-5.8.0/etc/init.d/cloudera-scm-agent start]]></content>
      <tags>
        <tag>cloudera</tag>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cloudera Manager配置]]></title>
    <url>%2F2016%2F09%2F07%2F2016-09-07_Cloudera%20Manager%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[登录http://&lt;cm-server&gt;:7180/cmf/login，用户名/密码：admin/admin。由于cm-server的启动需要花点时间，这里可能要等待一会才能访问。 选择Cloudera Express版本 在Currently Manage Hosts中选择所有节点 用Parcels安装，并选择合适的版本 由于我们已经下载了parcel并放到parcel-repo中，所以下载步骤很快就完成了。但分发，解压和激活还是需要一些时间 机器检测，正常情况应该都是绿勾 选择需要安装的服务 选择服务的集群分布，系统默认把所有的管理功能（比如hdfs namenode，hbase master等）放在cm-server，所有的slaves（datanode，region server等）功能放在其他cm-agent。“select hosts”表示暂不安装，未安装的服务可以以后随时安装 初始化数据库。建议为每种服务创建一个数据库 修改配置。一般不需要修改 集群初始化和启动，需要花一些时间 安装结束 管理服务 配置HDFS HA 配置Authentication 配置Autorization(sentry)]]></content>
      <tags>
        <tag>cloudera</tag>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[影响单次poll返回最大数据量的几个因素]]></title>
    <url>%2F2016%2F08%2F25%2F2016-08-25_%E5%BD%B1%E5%93%8D%E5%8D%95%E6%AC%A1poll%E8%BF%94%E5%9B%9E%E6%9C%80%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%8F%E7%9A%84%E5%87%A0%E4%B8%AA%E5%9B%A0%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[timeout 单次poll最大等待时间，consumer.poll(1000)意味着在缓存中的数据被消费完之后，consumer会到broker中拉取数据，最多等待1000毫秒。很明显，timeout越大，单次poll返回的数据就越多。但前提是有足够多的数据可供消费，如果在1000毫秒内，consumer已经拿完所有能拿的数据，就会立即返回。 max.poll.records 单次poll最大返回数据条数，默认值2,147,483,647。这应该不需要解释。 receive.buffer.bytes 用于TCP读取数据的缓存（SO_RCVBUF）大小，默认值64k。之前做过小测试，发现消费10万条大小1k的数据，花费5秒左右，设置max.poll.records=1，理论上来说性能应该大大下降才对，但实际测试结果也在5秒左右。后来发现消费端有64k缓存，换句话说每次TCP请求会拿64条数据，因此虽然poll每次只拿一条数据，但也是从缓存中拿，拿64次才会真正发送一次TCP请求。而相对于1k大小数据的消费，TCP请求的开销可以忽略，因此消费时间差不多。当测试数据大小为1个字节时，TCP请求的开销就不能忽略了，而测试结果也证实，max.poll.records=1会使消费性能下降2/3。 max.partition.fetch.bytes 单次poll对于每个partition最大返回数据量，默认值1M。如果消费的topic有3个partition，那么单次poll最多返回3M数据。但实际上发现单次poll一般都只返回1M的数据，而不是3M。检查数据的分布，也都分散在3个partition中。原因是consumer会启动3个fetcher分别到每个partition抓取数据，而考虑到性能问题，consumer会在fetch一个partition一定时间后才会fetch下一个partition，所以单次poll很可能只是在拿一个partition中的数据，所以返回1M数据。把测试时间延长之后发现，偶尔会有一次poll拿到2M数据，符合刚才的分析。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[log4j warnning]]></title>
    <url>%2F2016%2F08%2F23%2F2016-08-23_log4j%20warnning%2F</url>
    <content type="text"><![CDATA[12log4j:WARN Continuable parsing error 39 and column 23log4j:WARN The content of element type &quot;log4j:configuration&quot; must match &quot;(renderer*,appender*,plugin*,(category|logger)*,root?,(categoryFactory|loggerFactory)?)&quot;. log4j.xml的DTD规定log4j:configuration中的所有元素应该按照如下的顺序排列： 123456789101112131415&lt;renderer&gt;&lt;/renderer&gt;&lt;appender&gt;&lt;/appender&gt;&lt;plugin&gt;&lt;/plugin&gt;&lt;logger&gt;&lt;/logger&gt;&lt;category&gt;&lt;/category&gt;&lt;root&gt;&lt;/root&gt;&lt;loggerfactory&gt;&lt;/loggerfactory&gt;&lt;categoryfactory&gt;&lt;/categoryfactory&gt; 因此调整log4j.xml中的元素顺序即可避免该warnning]]></content>
      <tags>
        <tag>log4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加倍下注法Martingale测试]]></title>
    <url>%2F2016%2F08%2F17%2F2016-08-17_%E5%8A%A0%E5%80%8D%E4%B8%8B%E6%B3%A8%E6%B3%95Martingale%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[Martingale大致意思是 地下庄家开了一个赌局:猜硬币的正反。假设你下注1元，如果猜中，你可以赢1元，如果猜错，你要输1元。那有人想出来一个必胜法，先赌2元，赢了就收手。如果输了就赌4元。如果再输就赌8元，再输就赌16元，如此继续。这是必胜的赌博策略。 问，这个真的是赌博的必胜法吗？ 知乎上有很多讨论，结论是，本金足够的情况下，是必胜的，但是收益率很低，换句话说，这样赌博是不划算的。 所以写了段代码来模拟这个问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class TestGamble &#123; public static void main(String[] args) throws InterruptedException &#123; int win = 0; int lose = 0; // 赌博次数 int times = 10000; // 赌博总收益 long money = 0; // 单次赌博本金 long capital = 100 * 10000; // 期望收益率 double yield = 0.1; for (int i = 0; i &lt; times; i++) &#123; long result = gamble(capital, yield); if (result &gt; 0) &#123; win++; money += result; System.out.println("win!! money = " + money); &#125; else &#123; lose++; money -= capital; System.out.println("lose!! money = " + money); &#125; &#125; System.out.println("total win = " + win); System.out.println("total lose = " + lose); System.out.println("total money = " + money); System.out.println("total yield = " + money * 1.0 / (capital * times)); &#125; public static long gamble(long capital, double yield) throws InterruptedException &#123; long original = capital; // 下注额 long seed = 1; while (capital &gt; 0) &#123; if (dice()) &#123; capital += seed; System.out.println("seed = " + seed + ", win --- " + capital); &#125; else &#123; capital -= seed; System.out.println("seed = " + seed + ", lose --- " + capital); seed = Math.min(seed * 2, capital); &#125; if (capital &gt; original * (1 + yield)) &#123; return capital - original; &#125; &#125; return 0; &#125; public static boolean dice() &#123; return Math.abs((Math.random() + "").hashCode()) % 2 == 0; &#125;&#125; 测试结果如下 12345……total win = 8544total lose = 1456total money = 78812408total yield = 0.0078812408 也就是说，你准备了100亿来赌博，每次拿出100万来赌，一共赌10000次，每次赌博要么赢10%收手，要么输光。那么最终会赚7千多万，收益率0.79%，只是刚跑赢一年活期利率。把期望收益率调成50%或100%其实都差不多，只是输赢次数比例会有变化，收益率是差不多的。 总之，不划算。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kafka REST Proxy Content Types]]></title>
    <url>%2F2016%2F08%2F09%2F2016-08-09_Kafka%20REST%20Proxy%20Content%20Types%2F</url>
    <content type="text"><![CDATA[Kafka REST proxy的request和response的content type包含了数据的三种属性：序列化格式（比如json），API版本（比如v1）和内嵌格式（比如binary）。目前序列化格式只有json，API版本只有v1。 内嵌格式是指produce或者consume的数据的格式，这些数据本身是被内嵌在request或者response中。举个例子，你可以在json序列化的request中嵌入binary格式的数据，数据必须是base64编码的字符串。如果数据本身就是JSON，那么你可以以json格式直接嵌入。 Content Type的格式如下：1application/vnd.kafka[.embedded_format].[api_version]+[serialization_format] 当没有内嵌数据时，序列化格式是不能省略的。建议的Content Type是1application/vnd.kafka.[embedded_format].v1+json 但省略部分声明也是允许的，比如123application/vnd.kafka+jsonapplication/jsonapplication/octet-stream 这些Content Type没有声明API版本，默认使用最近的稳定版本。如果省略嵌入格式，默认使用binary格式。尽管以上Content Type都是允许的，但考虑到今后版本的兼容性问题，建议写全Content Type，并且检查response的Content Type Content negotiation也是支持的，所以你可以在Content Type中包含多个不同权重的设置1Accept: application/vnd.kafka.v1+json; q=0.9, application/json; q=0.5 当你想用一个新版本的API但又不确定它是否能使用的时候，这就很有用]]></content>
      <tags>
        <tag>kafka</tag>
        <tag>rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka REST Proxy主要功能]]></title>
    <url>%2F2016%2F08%2F08%2F2016-08-08_Kafka%20REST%20Proxy%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[Kafka REST Proxy的最终目标肯定是实现所有Java Client和命令行工具所有的功能，但目前只开发了或者说只开放了部分。 Metadata大部分关于brokers，topics，partitions和config的metadata都能通过GET请求获取 ProducersProducer API并不暴露producer对象，它创建了一个很小的producer池（每种格式一个producer），接受produce request并使用对应的producer来发送 ConsumersREST Proxy实现了high-level consumer的consumer-groups功能。Consumers是有状态的，因此需要跟REST Proxy的实例绑定。Offset的提交可以是自动的，也可以由client发送请求来提交。目前每个consumer都是单线程消费，所以如果希望通过多线程来提升吞吐量，那么需要使用多个consumer 数据格式REST Proxy目前支持三种格式：JSON，base64编码二进制和Avro。如果使用Avro，需要在Schema Registry中注册和验证schema 负载均衡REST Proxy可以启动多个实例来分担压力，并可以通过round robin DNS, 发现服务或者其他基于软件或硬件的负载均衡器来进行请求转发。但注意，由于consumer是有状态的，因此必须保证consumer必须跟REST Proxy实例绑定 Simple Consumer一般来说high-level consumer是推荐使用的，但如果有特殊的需求需要消费特定offset的消息，也可以使用Rest Proxy的low-level consumer]]></content>
      <tags>
        <tag>kafka</tag>
        <tag>rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka REST Proxy快速入门]]></title>
    <url>%2F2016%2F08%2F07%2F2016-08-07_Kafka%20REST%20Proxy%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Kafka REST Proxy是Confluent的一个组件，它提供了访问Kafka集群的RESTful接口。对于一些暂不支持Kafka的平台或编程语言来说，可以使用Kafka REST Proxy来发送和消费消息，查看集群状态和执行集群管理命令。 安装 要求Java版本 &gt;= 1.7 下载Confluent Platform安装包并解压 启动 cd confluent-&lt;version>/ 配置 修改etc/kafka-rest/kafka-rest.properties的zookeeper.connect为Kafka的zookeeper地址 因为我的应用并不需要发送/消费Avro格式的消息，也就不需要Schema Registry，所以将etc/kafka-rest/kafka-rest.properties的schema.registry.url配置项注释掉 启动，bin/kafka-rest-start etc/kafka-rest/kafka-rest.properties 发送JSON消息123curl -X POST -H &quot;Content-Type: application/vnd.kafka.json.v1+json&quot; \ --data &apos;&#123;&quot;records&quot;:[&#123;&quot;value&quot;:&#123;&quot;foo&quot;:&quot;bar&quot;&#125;&#125;]&#125;&apos; &quot;http://localhost:8082/topics/jsontest&quot; &#123;&quot;offsets&quot;:[&#123;&quot;partition&quot;:0,&quot;offset&quot;:0,&quot;error_code&quot;:null,&quot;error&quot;:null&#125;],&quot;key_schema_id&quot;:null,&quot;value_schema_id&quot;:null&#125;发送的消息体必须是key为records的JSON，每个record可以有key和value属性，其中value是必须的消费JSON消息 创建Consumer实例 12345curl -X POST -H &quot;Content-Type: application/vnd.kafka.v1+json&quot; \ --data &apos;&#123;&quot;name&quot;: &quot;my_consumer_instance&quot;, &quot;format&quot;: &quot;json&quot;, &quot;auto.offset.reset&quot;: &quot;smallest&quot;&#125;&apos; \ http://localhost:8082/consumers/my_consumer_group &#123;&quot;instance_id&quot;:&quot;my_consumer_instance&quot;, &quot;base_uri&quot;:&quot;http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance&quot;&#125; 使用已创建的Consumer实例消费消息12$ curl -X GET -H &quot;Accept: application/vnd.kafka.json.v1+json&quot; \ http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance/topics/jsontest [&#123;&quot;key&quot;:null,&quot;value&quot;:&#123;&quot;foo&quot;:&quot;bar&quot;&#125;,&quot;partition&quot;:0,&quot;offset&quot;:0&#125;] 删除Consumer实例1curl -X DELETE \ http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance]]></content>
      <tags>
        <tag>kafka</tag>
        <tag>rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka机架感知]]></title>
    <url>%2F2016%2F08%2F05%2F2016-08-05_Kafka%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[从狭义字面上理解，机架感知的意思是Kafka会把partition的各个replicas分散到不同的机架上，以提高机架故障时的数据安全性。假如所有replicas（包括leader）都在一个机架上，那么当这个机架发生故障时，所有这个机架上的server都不能提供服务，就会发生数据丢失，而多个机架同时发生故障的概率则要小得多。 从广义上来讲，这实际上是个broker分组功能，可以将不同组的brokers分散到不同的区域中，以提高单个区域发生故障时整个集群的可用性。比如碰到有个用户，他们公司的网络分成若干个分区，他们希望当单个网络分区故障，或者跟其他分区无法通信时，Kafka仍旧能够保证正常工作，这就可以用到broker分组功能。 你可以通过修改server.properties来指定broker属于哪个特定的组（机架）： broker.rack=rack-id-n 当创建，更新或者replicas重新分布时，将会遵从分组规则，确保单个partition的所有replicas被分散到尽可能多的组内，最多会被分散到min(#racks, replication-factor) 个不同的组。 Kafka的replicas分配算法确保每个broker的leader数目都是恒定的，而不取决于如何分组。这保证了整个集群的吞吐量负载均衡。但如果不同的组被分配了不同数量的broker的话，则每个组的总备份数不一定是平均的。因为broker少的那些组将会分配到更多的replicas，这意味着这些broker会有更大的磁盘和其他用于replicas的资源开销。因此比较明智的做法是给每个组都分配相同数量的broker。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Kafka生产中使用SASL]]></title>
    <url>%2F2016%2F07%2F31%2F2016-07-31_%E5%9C%A8Kafka%E7%94%9F%E4%BA%A7%E4%B8%AD%E4%BD%BF%E7%94%A8SASL%2F</url>
    <content type="text"><![CDATA[在之前一篇文章中介绍了Kakfa0.10中提供的SASL/PLAIN安全认证机制，这种认证使用起来非常简单，但如果直接应用到生产环境中，就不够严谨了。但好在Kafka提供了接口可以让我们对其进行优化和增强。 为了保证不会在网络中明文传输密码，SASL/PLAIN应该使用SSL(TLS)作为传输层协议 在Kafka client中，默认的SASL/PLAIN登录模式是在JAAS配置文件中指定用户名和密码。如果要避免把密码保存在磁盘上，可以自己实现javax.security.auth.spi.LoginModule来通过其他方式提供用户名和密码。新实现的登录模块必须提供用户名作为public credential，密码作为private credential，设置到subject中 在生产系统中，可以通过添加javax.security.sasl.SaslServer的实现来提供额外的认证server。默认的实现在Kafka的org.apache.kafka.common.security.plain包中。 新的Providers必须安装和注册在JVM中。安装的话，可以直接将Providers的classes加入到CLASSPATH中，或者打成jar包并加入JAVA_HOME/lib/ext 可以将Providers注册到security配置文件JAVA_HOME/lib/security/java.security中：security.provider.n=providerClassName，其中providerClassName是provider的全名（包括包名），n指的是优先权，n越小认证优先权越大 除此之外，你也可以在运行时在应用程序开始时调用Security.addProvider来注册provider，例如Security.addProvider(new PlainSaslServerProvider());]]></content>
      <tags>
        <tag>kafka</tag>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ScheduledExecutorService.scheduleAtFixedRate精确性]]></title>
    <url>%2F2016%2F07%2F26%2F2016-07-26_ScheduledExecutorService.scheduleAtFixedRate%E7%B2%BE%E7%A1%AE%E6%80%A7%2F</url>
    <content type="text"><![CDATA[先说结论，ScheduledExecutorService.scheduleAtFixedRate只能保证执行的频率，并不能精确保证两次执行的时间间隔。 之前有个定时数据采集的任务，使用ScheduledExecutorService.scheduleAtFixedRate每隔10秒钟触发一次，将数据采集并保存下来。为了便于查询，以触发采集的时间（秒）向下取整作为主键，例如当前时间08:21:18触发，那么这次采集的数据的key就是08:21:10。 后来发现偶尔会有某个时间点的数据没有采集到，而某个时间点又会采集两次，比如11:27:20，11:27:30，11:27:50，11:27:50（缺少11:27:40）。 经过排查，发现任务启动的时间是11:27:29，所以理论上来说，应该在11:27:39，11:27:49，11:27:59都会触发一次，但从日志上看，实际上的触发时间是11:27:29，11:27:39，11:27:50，11:27:59，其中本该在11:27:49的采集任务延迟了一秒才被触发。 这个问题本质的原因是ScheduledThreadPool利用一个DelayQueue来保存下一次需要被触发的任务。DelayQueue使用System.nanoTime()来计算任务还有多久被触发。 在x86系统中，System.nanoTime()是跟CPU内置的counter紧密相关，那么问题来了, 在一个多核的操作系统中，第一次任务可能被CPU core1安排，第二次任务可能被CPU core2安排，两个核的counter很有可能是不同的，那么两次任务的System.nanoTime()相减就不能保证是精确的10秒钟。]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka0.10升级潜在影响（二）]]></title>
    <url>%2F2016%2F07%2F05%2F2016-07-05_Kafka0.10%E5%8D%87%E7%BA%A7%E6%BD%9C%E5%9C%A8%E5%BD%B1%E5%93%8D%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[接上篇，今天做了个测试，看log.message.format.version对consumer性能的影响到底有多大，一共20万条数据，每条数据大小1K，总共200M。 首先在Kafka 0.9上的平均测试结果是8.362秒。 而升级到Kafka 0.10之后，平均测试结果如下： producer versionlog.message. format.versionconsumer versioncost0.9 0.10 0.9 8.966s 0.9 0.10 0.10 6.470s0.100.10 0.9 8.834s0.100.10 0.106.604s0.90.90.98.355s0.90.90.106.242s0.100.90.98.233s0.100.90.106.310s 对比1-5，2-6，3-7，4-8，使用默认的log.message.format.version=0.10.0会使吞吐量降低，因为消息里增加了8个字节timestamp，虽然原始size是1K，只增加了8‰的大小，但从测试结果看，吞吐量降低远远不止8‰ 。 设置log.message.format.version=0.9，吞吐量和kafka0.9差不多。这个很好理解。 producer的版本不影响消费吞吐量。无论produce的message是什么版本，保存到broker之后都以log.message.format.version为准，因此consume的性能是不受影响的。 0.10的consumer吞吐量大大优于0.9consumer， 结论： 无论如何，kafka broker升级到0.10之后，优先使用0.10 consumer，效率高 从性能上考虑，应该设置log.message.format.version=0.9，但这样就不能使用0.10的message的新特性，比如timestamp]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka0.10升级潜在影响（一）]]></title>
    <url>%2F2016%2F07%2F04%2F2016-07-04_Kafka0.10%E5%8D%87%E7%BA%A7%E6%BD%9C%E5%9C%A8%E5%BD%B1%E5%93%8D%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Kafka 0.10的消息格式有新的变化，加入了timestamp字段。磁盘存储的消息格式可以通过server.properties的log.message.format.version配置，默认值是0.10.0.0。 如果一个consumer的版本低于0.10，那么它只能解析0.10以前的数据格式。在这种情况下，Kafka broker会为这个低版本consumer将数据转换到它能够解析的低版本格式。但同时broker将不能使用zero-copy传输，因为数据必须读到application cache中进行数据格式转换。为了避免这种转换开销，我们可以在broker升级到0.10之后，将数据声明为更低版本的格式，比如0.9.0.1。这样，当老版本的consumer尝试获取数据时，broker能够使用zero-copy直接将数据传输给consumer。当大多数consumer都升级到最新版本之后，我们可以将消息格式重新声明称0.10（默认值）。而对于已经升级到0.10的client来说则不受影响。 另外有一些需要注意的是： 在设置log.message.format.version时，必须保证所有的存量数据必须是不高于设置的消息格式。否则低版本的consumer会中断。尤其需要注意在消息格式在设置为0.10之后，不能再设置回低于0.10的版本。 由于timestamp字段被加入消息中，消息本身的开销会增加，因此producer的吞吐量会降低。如果消息本身的size很小，那么新字段对于开销的增加会很明显，吞吐量会明显降低；而相对来说如果消息的size比较大，性能影响就不那么大。同样的，消息的replication也同样会增加8个字节的开销。如果Kafka已经快要把网络压榨到极限，那么这个改动可能会成为最后一根稻草，你可能会遇到一些报错和性能问题。 如果已经在producer中使用了压缩机制，那么你可能会感觉到性能相比之前有所降低，又或者是数据压缩率降低。在0.10中，当broker收到压缩的消息时，它会避免再将消息压缩，一般来说这可以降低延迟和提高吞吐量。但在特定情况下，这可能反而会降低producer的batch size，从而使吞吐量进一步降低。如果碰到这种情况，可以调整producer的linger.ms和batch.size。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka安全认证-SASL]]></title>
    <url>%2F2016%2F07%2F02%2F2016-07-02_Kafka%E5%AE%89%E5%85%A8%E8%AE%A4%E8%AF%81-SASL%2F</url>
    <content type="text"><![CDATA[Kafka0.10提供SASL/PLAIN，这是一种简单的用户名/密码安全认证机制。 配置Kafka Brokers 创建kafka_server_jaas.conf 1234567KafkaServer &#123; org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;admin&quot; password=&quot;admin-secret&quot; user_admin=&quot;admin-secret&quot; user_alice=&quot;alice-secret&quot;;&#125;; 这里定义了两个用户，admin和alice。username和password用于集群内部通讯，即当前broker使用admin/admin-secret去连接别的broker。而user_userName=”password”定义了能够访问这个broker的用户及其密码。 将kafka_server_jaas.conf拷贝到Kafka的所有broker上 将kafka_server_jaas.conf加入到Kafka broker启动参数中 1-Djava.security.auth.login.config=$&#123;PATH&#125;/kafka_server_jaas.conf 在server.properties中配置SASL端口和SASL实现机制，例如 1234listeners=SASL_PLAINTEXT://localhost:9092security.inter.broker.protocol=SASL_PLAINTEXTsasl.mechanism.inter.broker.protocol=PLAINsasl.enabled.mechanisms=PLAIN 配置Kafka Clients 创建kafka_client_jaas.conf，consumer和producer使用它来访问Kafka broker12345KafkaClient &#123; org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;alice&quot; password=&quot;alice-secret&quot;;&#125;; 属性username和password被用于配置到Kafka broker的连接。在这个例子中，clients使用用户alice连接broker 将kafka_client_jaas.conf加入到Kafka client启动参数中: 1-Djava.security.auth.login.config=$&#123;PATH&#125;/kafka_client_jaas.conf 在producer.properties或者consumer.properties中加入以下配置： 12security.protocol=SASL_PLAINTEXTsasl.mechanism=PLAIN]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka多数据中心架构设计（二）]]></title>
    <url>%2F2016%2F06%2F30%2F2016-06-30_%20Kafka%E5%A4%9A%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[用mirrormaker在多个数据中心做Kafka的数据同步还需要考虑几个问题： 如何提高同步效率？ 跨数据中心的数据传输，瓶颈在于带宽。我们采取的办法是启多个mirrormaker进程和压缩。 在两台机器间带宽固定的情况下，理论上在多台机器上启多个mirrormaker进程能扩大总带宽。但在实际应用中的效果，还要看具体的网络配置 Kafka本身支持GZIP，Snappy和LZ4压缩协议。从我的测试结果来看，LZ4压缩和解压效率最高，GZIP压缩率最高，具体还是要看数据格式。 mirrormaker部署在哪？ 将数据从数据中心A同步到数据中心B，mirrormaker部署在A端（Source端）或者B端（Destination端）都可以，但还是会有点区别。 LinkedIn建议部署在Destination端。对于跨数据中心的数据同步来说，最不稳定的因素就是网络，我们需要保证的，是在网络断开的情况下不丢数据或者丢尽量少的数据。mirrormaker本质上是consumer + cache + producer，offset在放进cache之后就commit，因此mirrormaker部署在Destination端的好处显而易见，假如数据中心间的网络出了问题，mirrormaker根本就收不到数据，也就不存在丢数据的问题；但如果部署在Source端，网络出问题之后，mirrormaker consumer仍然能够从本地Kafka集群获取数据放进cache中，并commit offset，但数据并没有成功的produce到Destination端，mirrormaker也不再有机会重新获取数据。 我个人建议是部署在Source端。原因很简单，为了压缩。我们压缩的目的是为了数据在producer远程传输的过程中能尽量的减少带宽占用，如果部署在Destination端，mirrormaker consumer消费的是原始数据，并没有节省带宽。 如果数据本身是压缩过的，mirrormaker怎么处理？ 理想情况下，我们应该直接把压缩的数据传输到远端，但现在mirrormaker内置的consumer会在消费后先把数据解压缩，然后producer发送前再把数据压缩。 这样对CUP的消耗很大且不必要，当至少到现在为止Kafka还没有给出解决办法。 如何保证数据有序？ 只有满足以下条件才能保证partition内数据有序： 同步的数据是keyed的 mirrormaker和数据原来的producer使用相同的partitioner Source topic和Destination topic的partition数目相同 双向同步？ mirrormaker不支持相同topic的双向同步。如果这么做了，会导致数据在两个Kafka集群之间无限的重复同步，因为mirrormaker并不能判断一条数据是被普通producer发进来的还是之前被它自己从另一个数据中心同步过来的。 我的建议是，用上一篇文章说的聚合集群。如果不想这么大动干戈，也可以用不同的topic后缀来区分，比如topicA_sh，topicA_bj，保证数据不会被重复同步。 或者有个更简单但不是很合理的做法。强制在所有的数据的key中划分出一段来专门记录数据被发送到的数据中心名字，然后在MirrorMakerMessageHandler中解析key，判断是否需要同步。比如如果发现一条数据来自shanghai，那么beijing到shanghai的mirrormaker不会对它做同步。这种做法对数据格式有很强的侵入性，所以不是很建议。其实个人认为如果给kafka record本身加上metamap就能解决这个问题。]]></content>
      <tags>
        <tag>kafka</tag>
        <tag>mirrormaker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka多数据中心架构设计（一）]]></title>
    <url>%2F2016%2F06%2F25%2F2016-06-25_Kafka%E5%A4%9A%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[将Kafka部署在单一数据中心是一种相对比较常见的架构，所有的broker，producer和consumer都在同一个数据中心内，一般来说，也都在同一个内网里。 但考虑这样一个问题，公司在北京，上海和深圳都有业务系统，其中上海和北京会生成一些数据或日志，并且这些信息都需要被三地的业务系统所使用。假如我们考虑用Kafka来作为消息系统，那么就不得不考虑多数据中心的部署问题。 上图是一个跨数据中心使用一个kafka集群的例子。我们不会搭建一个跨数据中心的集群，因为这会带来极高的复杂度。此外，kafka不能读replicas，因此即使集群做到跨数据中心，clients仍旧不能保证本地读。 但是很明显，这个架构的问题在于，如果kafka集群所在的数据中心挂掉了，那么所有的数据都丢了。 为了解决这些问题，我们可以在A和B两个主要的数据中心各搭建一个kafka集群。而对于像C这样的只需要consume而不需要produce的数据中心，我们通常把它作为一个后端的环境，来跑一些离线的应用。 在这个设计中，所有的producer都向本地发送数据，数据中心A和B的consumer从本地消费数据。如果A或者B挂掉了，另一个数据中心仍旧能工作。但这么也需要付出一定的代价，如果consumer需要获取所有的数据，就必须连接到所有的数据中心，并且需要处理跨数据中心的网络的连接和延迟问题。 为了把这个模型再优化一下，我们增加了聚合集群的概念，这个集群包含了所有其他集群聚合起来的数据。对于消费者来说，他们只需要在本地就能从聚合集群中拿到所需的数据。当然，我们仍然有跨数据中心的网络问题，但这其实是无法避免的，毕竟我们需要把数据从一个数据中心拷贝或移动到另一个数据中心。我们可以把这些事情都集中到一个应用中完成，这个应用叫做mirrormaker，然后我们可以对mirrormaker进行集中的监控以确保它正常工作。这样对于每个consumer来说，他们就不需要关心消费的网络和延迟问题了。 尽管这种架构更加复杂，但对于client来说，他们使用kafka会变得更简单。对于producer，他们只需要往本地集群中发数据，而不需要关心数据会存放到何处；对于consumer，只要选择一个聚合集群就能获取所有数据。而其他的所有问题，比如数据如何流转，如何传输，都将交给mirrormaker来处理。]]></content>
      <tags>
        <tag>kafka</tag>
        <tag>mirrormaker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Rest Consumer性能优化]]></title>
    <url>%2F2016%2F06%2F17%2F2016-06-17_Kafka%20Rest%20Consumer%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[之前有多语言consume kafka的需求，因此决定采用Confluent的Kafka Rest2.0.1。虽然测试的结果不如人意（java consumer大约60M/s，但用rest的话只有5M/s）,考虑到消息量不大，也能凑合用 但最近在kafka集群中新创建了200个测试的空topic，rest consumer的性能急剧降低，惨不忍睹。。。 下载了源码debug了一下，发现每次consume都需要判断一下topic是否存在 123456789101112131415161718public List&lt;Topic&gt; getTopics() &#123; try &#123; Seq&lt;String&gt; topicNames = zkUtils.getAllTopics().sorted(Ordering.String$.MODULE$); return getTopicsData(topicNames); &#125; catch (RestNotFoundException e) &#123; throw new InternalServerErrorException(e); &#125; &#125; public boolean topicExists(String topicName) &#123; List&lt;Topic&gt; topics = getTopics(); for (Topic topic : topics) &#123; if (topic.getName().equals(topicName)) &#123; return true; &#125; &#125; return false; &#125; MetadataObserver.getTopics()耗时非常长，该函数先从zookeeper中取出所有的topicName，然后再到zookeeper中把所有的topic的metaData查询出来封装成Topic对象然后返回。增加了200个topic之后，获取所有topic的metaData变得非常的耗时间，其实这是完全没有必要的，直接将Seq topicNames返回就行了，因为判断topic是否存在只需要比较topicName。 因此代码改成 1234567891011121314public Collection&lt;String&gt; getTopicNames() &#123; Seq&lt;String&gt; topicNames = zkUtils.getAllTopics().sorted(Ordering.String$.MODULE$); return JavaConversions.asJavaCollection(topicNames); &#125; public boolean topicExists(String topicName) &#123; List&lt;String&gt; topics = Lists.newArrayList(getTopicNames()); for (String topic : topics) &#123; if (topic.equals(topicName)) &#123; return true; &#125; &#125; return false; &#125; 那么，rest consumer为什么每次消费都需要先判断Topic是否存在呢？ rest是无状态的，但kafka consumer必须要保存状态，因此必须在rest服务器端缓存consumer instances，每个consume rest请求都是通过缓存的instance向kafka server请求数据。 rest consumer为了限制每次请求返回的数据量，避免客户端内存爆掉，提供了consumer.request.timeout.ms参数，默认值1000ms，意味着无论instance是否从kafka server端拿到了数据，1秒钟后必须给rest 客户端响应，如果没拿到数据，就返回空数组。 那么问题来了，假如topic不存在，consumer instance会获取topic的metadata超时，但在超时之前，就已经超过了1秒，空数组会被返回给rest client。事实上，rest client的每次消费请求都会返回空数组，但它只知道没有拿到数据，并不知道是topic不存在，还是真的没有数据被produce进来。 为了避免这种情况发生，必须处理每个consume请求时先判断topic是否存在。 顺带说一句，java client也应该提供判断topic是否存在的api啊。。。不能让client等这么久最后返回个failed to update metadata after 60000ms…能不能友好一点。。 ———————————- 我是分割线 ——————————————– 注：该BUG在3.0版本已被官方修复。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Consumer Session Timeout]]></title>
    <url>%2F2016%2F06%2F13%2F2016-06-13_Kafka%20Consumer%20Session%20Timeout%2F</url>
    <content type="text"><![CDATA[在kafka client（consumer）的log中看到这样的异常： 12[13/06/16 16:01:26:153 PM CST] 550 ERROR ConsumerCoordinator: Error ILLEGAL_GENERATION occurred while committing offsets for group group7`[13/06/16 16:01:26:154 PM CST] 424 WARN ConsumerCoordinator: Auto offset commit failed: Commit cannot be completed due to group rebalance 检查server端的log 12345[2016-06-13 04:01:16,458] INFO [GroupCoordinator 1]: Preparing to restabilize group group7 with old generation 0 (kafka.coordinator.GroupCoordinator)[2016-06-13 04:01:16,458] INFO [GroupCoordinator 1]: Stabilized group group7 generation 1 (kafka.coordinator.GroupCoordinator)[2016-06-13 04:01:16,462] INFO [GroupCoordinator 1]: Assignment received from leader for group group7 for generation 1 (kafka.coordinator.GroupCoordinator)[2016-06-13 04:01:46,469] INFO [GroupCoordinator 1]: Preparing to restabilize group group7 with old generation 1 (kafka.coordinator.GroupCoordinator)[2016-06-13 04:01:46,470] INFO [GroupCoordinator 1]: Group group7 generation 1 is dead and removed (kafka.coordinator.GroupCoordinator) Kafka consumer group有generation（代）的概念，一个group开始消费之后，被标记为G代，由于一些原因，G代过期，该group被标记为G+1代，这时如果G代的group尝试commit offset，就会发生本文开头出现的错误。 那么什么会导致generation过期呢？ Rebalance 显然rebalance前后，consumer和partition之前的消费关系已经发生了变化，如果generation不过期，会发生offset的误提交 session timeout 每次consumer.poll的调用都会向server端发送心跳，如果server端在session.timeout.ms（默认30s）时间内没有收到心跳，则认为该consumer group已经断开或者shutdown 经排查，我们这次问题的原因是每次poll会拿到一万多条数据，这些数据的处理需要耗费大概60s，所以当consumer执行下一次poll并commit offset时，server端发现该group的generation已经过期，因此报错。 解决方案是修改consumer的参数session.timeout.ms为120s，或者限制每次poll最多只拿2000条数据（max.poll.records， kafka1.0）]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka0.10新特性]]></title>
    <url>%2F2016%2F05%2F26%2F2016-05-26_Kafka0.10%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[5月23日Confluent官方宣布Apache Kafka 0.10正式发布。该版本包含了很多新功能和优化，这里列出比较重要的几项： Streams如果你有这样的需求，从Kafka拉取数据进行流处理然后再推送回Kafka，那么你会喜欢0.10的Kafka Streams。Kafka Streams是一个类库，它实现了一系列流处理动作（例如join，filter，aggregate等），能够帮助你构建一个功能齐全的低延迟的流处理系统。它支持有状态或无状态的处理，并且能够被部署在各种框架和容器中（例如YARN，Mesos，Docker），也可以集成在Java应用里。 机架感知和Hadoop一样，Kafka现在也实现了机架感知。如果所有备份都在单个机架上，那么一旦这个机架出问题，那么所有的备份都将失效。现在Kafka会让备份分布在不同的机架上，显著的提高了可用性。 Message中加入Timestamp在Message中加入了Timestamp，如果没有被用户声明，该字段会被自动设为被发送的时间。这使得Kafka Streams实现了基于时间事件的流处理，你也可以使用Timestamp来实现消息的追踪查找。除次之外Message中还加入了checksum（但并不是保存在Kafka中，只是取出来之后计算），可以以比较小的代价比对Message。 SASL增强Kafka0.9提供了SASL/Kerberos，在0.10中增加了更多的SASL功能，比如SASL/Plaintext Kafka Connect Rest API在之前的版本中，用户只能通过log来监控Connector的状态。在0.10中增加了监控和控制的API，可以列出所有的Connector状态，并且可以暂停或重启任务。 Kafka Consumer Max Record在0.9中，如果想要控制Consumer的单次请求返回数据量，只能控制timeout的大小，0.10加入新的Consumer参数max.poll.records来控制返回的数据条数。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka MirrorMaker实践]]></title>
    <url>%2F2016%2F04%2F20%2F2016-04-20_Kafka%20MirrorMaker%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[最近准备使用Kafka Mirrormaker做两个数据中心的数据同步，以下是一些要点： mirrormaker必须提供一个或多个consumer配置，一个producer配置，一个whitelist或一个blacklist（支持java正则表达式） 启动多个mirrormaker进程，单个进程启动多个consuemr streams， 可以提高吞吐量和提供容 mirrormaker部署在destination datacenter，这样如果kafka集群之间发生网络问题，也不至于从src cluster拿到了数据但发不到dest cluster导致数据丢失 mirrormaker不能防止数据循环发送，即如果使用mm将数据从ClusterA的TopicA复制到ClusterB的TopicA，另一个mm将数据从ClusterB的TopicA复制到ClusterA的TopicA，那么会产生endless loop，mm的负载会急剧上升 mirrormaker的producer和consumer的一些配置的目标是数据不丢失，而不是高性能，它们分别是 acks=all(kafka consumer默认1), 意味着数据被拷贝到dest cluster的所有replicas之后才响应 retries=max integer(kafka producer默认0) block.on.buffer.full=true(kafka produmer默认false) max.in.flight.requests.per.connection=1(kafka producer默认5), 提升该值可以获得更快的速度，同时意味着如果mirrormaker挂掉，将会丢更多的数据 auto.commit.enable=false(默认true) abort.on.send.failure=true(mirrormaker配置) 其他配置：linger.ms=0（kafka producer默认0）, 调高linger.ms会使mirrormaker能够将更多的消息打包发送以提升效率，同时意味着消息的平均延迟上升 可以给所有需要mm的topics设置优先级，优先级高的topic将获得更低的延迟，并且能在更短的时间内重启，重启之后也能更快的追上拷贝进度]]></content>
      <tags>
        <tag>kafka</tag>
        <tag>mirrormaker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KafkaConsumer0.9（三）]]></title>
    <url>%2F2016%2F01%2F18%2F2016-01-18_KafkaConsumer0.9%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test_group"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("session.timeout.ms", "30000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); List&lt;TopicPartition&gt; list = new ArrayList&lt;TopicPartition&gt;(); TopicPartition tp = new TopicPartition("test_topic", 0); list.add(tp); consumer.assign(list); consumer.seek(tp, 96); // consumer.seekToBeginning(tp); // consumer.seekToEnd(tp); int commitInterval = 200; List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = new ArrayList&lt;ConsumerRecord&lt;String, String&gt;&gt;(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; buffer.add(record); if (buffer.size() &gt;= commitInterval) &#123; batchProcessRecords(buffer); consumer.commitSync(); buffer.clear(); &#125; &#125; &#125; 在上一篇中我们看了一个简单的自动提交offset的example，但很多情况下我们为了避免消息丢失，需要确保消息被处理完了之后才提交offset，这就需要手动地提交。在上面这个例子中，我们从kafka中抓取数据并缓存在List中，只有当消息达到一定的数量的时候我们才批量处理，假设我们使用自动提交，如果在我们还没来得及处理之前consumer就异常终止，那么有可能这些消息的offset已经被自动提交掉了，等我们的consumer重新连接上来了之后，上次没有处理完成的消息会被我们完全略过，造成数据丢失，这就是”at-most-once delivery”。解决的办法是，只有在批量处理完消息之后，才用consumer.commitSync()手动地提交offset，但这样的副作用的，假如我们正在批量处理消息，这时consumer异常终止，offset没有被提交但有部分消息已经被处理过了，当consumer重连上来时，这批没有被commit的消息会被重新处理一次，造成会有部分消息被重复处理，这就是”at-least-once delivery”。 kafka提供load balance机制来确保consumer正常工作，简单的说partitions会被分配给正在监听这个topic的多个consumers（同一个group），当其中一个consumer process异常终止，它之前所占有的partitions会被分配给其他consumer process，从而保证所有的数据都能被正常消费掉。但有时我们并不需要load balance机制，例如： * 为了节省网络带宽，我们只希望consumer从某一个partition抓取数据，并存储在本地。这在大数据计算或存储中是很常见的行为。在这种情况下我们并不希望另一台机器的consumer来消费这台机器的partition。 * 如果程序本身带有HA机制，例如使用类似于YARN,Mesos等集群管理框架，那么当一个consumer终止了之后，它会被重启，或者是另一个consumer会被启动来替代它，在这种情况下我们不需要kafka重新分配partition。 要做到这点很简单，替换掉上个例子的consumer.subscribe(Arrays.asList(“test_topic”))，我们使用consumer.assign(list)，在本例中，consumer只会消费partition0的数据。 在之前的版本中，如果我们需要消费旧数据（已经commit offset），我们需要用SimpleConsumer。但是在0.9中，这变得更简单了。在本例中，consumer.seek(tp, 96)表示我们从partition 0的offset 96开始抓取数据，consumer.seekToBeginning(tp)表示从头开始抓取数据，consumer.seekToEnd(tp)表示从最后开始抓取数据，换句话说，只消费consumer启动之后新进来的数据。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KafkaConsumer0.9（二）]]></title>
    <url>%2F2016%2F01%2F13%2F2016-01-13_KafkaConsumer0.9%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[先看一个简单的KafkaConsumer例子： 12345678910111213141516Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test_group"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("session.timeout.ms", "30000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); consumer.subscribe(Arrays.asList("test_topic")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records)&#123; System.out.printf("offset = %d, key = %s, value = %s", record.offset(), record.key(), record.value()); &#125; &#125; 我们看到0.9的consumer最大的变化是： 通过consumer.subscribe(Arrays.asList(“test_topic”))来声明要订阅的topic，而之前的版本是用Whitelist声明。 通过consumer.poll(100)直接抓取消息，而之前需要遍历KafkaStream的迭代器。（这个比之前方便太多了。。。）。 MessageAndMetadata变成了ConsumerRecords enable.auto.commit表示已一个固定的时间间隔自动提交offsets，时间间隔由auto.commit.interval.ms控制。 bootstrap.servers表示kafka集群的broker列表。客户端会连接到这个列表中的任意一台机器，获取到整个集群的信息，因此理论上只需要在bootstrap.servers列出一台机器就够了，但是考虑到灾备，建议在bootstrap.servers中包含所有broker。 key.deserializer和value.deserializer指定了如何解析记录的key和value，在本例中我们认定key和value都是字符串。 在本例中，client端启动了一个从属于test_group的consumer来订阅test_topic。当其中一个consumer process断开之后，kafka broker会通过心跳机制自动检测到，因此集群始终能够知道哪些consumer是活着的。只要被认为是活着，这个consumer就能够从分配给它的partition中获取数据；一旦心跳丢失超过session.timeout.ms，consumer会被认为死掉，它所占有的partition将会被分配给别的process。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KafkaConsumer0.9（一）]]></title>
    <url>%2F2016%2F01%2F12%2F2016-01-12_KafkaConsumer0.9%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Kafka0.9发布了新版consumer client。它与旧版本最大的区别是移除了基于zookeeper的high-level consumer和low-level SimpleConsumer，而代之于一个统一的consumer API，它集成了之前high-level consumer的group管理功能和low-level consumer的offset控制功能。 新的consumer实现了一套新的group管理机制，它使得consumer clients变得更简洁（真的比以前简洁很多。。。），并且获得更快的rebalancing。同时这个版本也完全解除了consumer client对zookeeper的依赖，直接访问kafka server使得consumer变得简单，并且能够使用到kafka server提供的安全（security）和配额机制(quota)。 这里多说一句，consumer不依赖zookeeper是对的，由于zookeeper本质上是一个cp而不是ap系统，它更适于用于服务协调而不是服务发现，因为对于后者来说，信息中可能包含错误总比没有信息要好。 为了让用户平滑地升级，0.8的consumer依然可以连接到0.9的kafka cluster。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Connect]]></title>
    <url>%2F2016%2F01%2F11%2F2016-01-11_Kafka%20Connect%2F</url>
    <content type="text"><![CDATA[Kafka Connect是Kafka0.9新增的模块。可以从名称看出，它可以和外部系统、数据集建立一个数据流的连接，实现数据的输入、输出。有以下特性： 使用了一个通用的框架，可以在这个框架上非常方面的开发、管理Kafka Connect接口 支持分布式模式或单机模式进行运行 支持REST接口，可以通过REST API提交、管理 Kafka Connect集群 offset自动管理 在大部分Kafka应用场景中，我们常常需要从某一数据源导入数据到Kafka中，或者将Kafka中的数据导出（准确说是被取出）到其他系统中。为了实现这一功能，我们一般需要在上游系统中创建一个Kafka Producer，或在下游系统中创建Kafka Consumer。 在Kafka0.9中新增的Kafka Connect模块实现了以上功能，这样我们就可以省掉了一些通用交互代码，而只需要做简单的配置就行了。这和Logstash的思路很相似，Kafka提供了connector的接口，通过实现该接口来，我们可以创建各种各样的Input和Output插件。当然Kafka Connect才刚刚推出，插件远没有Logstash丰富，不过相信随着Kafka0.9的普及，这一功能将变得更加实用。 以下介绍Kafka Connect的一个简单的实现，FileInput和FileOutput，启动方式如下： 1bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties 执行connect-standalone.sh会在本地单机启动connector(s)。我们必须声明至少两个参数，第一个是Kafka Connect的配置文件，包含一些基本配置例如kafka server地址，序列化格式等等；其他的配置文件用于配置connector(s)，每个配置文件创建一个connector。我们来看一下这两个配置文件： connect-file-source.properties12345name=local-file-sourceconnector.class=org.apache.kafka.connect.file.FileStreamSourceConnectortasks.max=1file=/tmp/test-src.txttopic=test 该配置会启动一个名叫local-file-source的connector，这个connector会从/tmp/test-src.txt中读取数据，然后写入名叫test的topic中。 connect-file-sink.properties12345name=local-file-sinkconnector.class=org.apache.kafka.connect.file.FileStreamSinkConnectortasks.max=1file=/tmp/test-dest.txttopics=test 该配置会启动一个名叫local-file-sink的connector，这个connector会从Kafka的test topic中读取数据，然后写入/tmp/test-dest.tx文件中。 所以我们看到，运行以上命令实际上是启动了一个程序不断地读取/tmp/test-src.txt，并通过Kafka Producer发送到Kafka 的test这个topic中，同时启动一个Kafka Consumer从这个topic中读取数据并写入/tmp/test-dest.txt，启动之后我们可以测试一下。 1echo &quot;hello world&quot; &gt;&gt; /tmp/test-src.txt 我们往/tmp/test-src.txt中写入几个字符，然后我们查看/tmp/test-dest.txt，发现“hello world”已经被写入/tmp/test-dest.txt中。 多说一句，数据并不是以文本的形式存入Kafka，而是Json。它的格式类似于： 1&#123;"schema":&#123;"type":"string","optional":false&#125;,"payload":"hello world"&#125; 所以本文中两个connector实际上还做了数据的封装和Json解析的工作。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch配置内存]]></title>
    <url>%2F2015%2F11%2F11%2F2015-11-11_Elasticsearch%E9%85%8D%E7%BD%AE%E5%86%85%E5%AD%98%2F</url>
    <content type="text"><![CDATA[原生ES，只需要在启动时加上-Xmx1g -Xms1g Elasticsearch Docker，参数为 -eES_MIN_MEM=1g-eES_MAX_MEM=1g 注意： mx和ms最好设为一样，避免GC 为了保证最大效率，ES内存设为预留内存的一半，另外一半留给Lucene。]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch删除所有数据]]></title>
    <url>%2F2015%2F11%2F10%2F2015-11-10_Elasticsearch%E5%88%A0%E9%99%A4%E6%89%80%E6%9C%89%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[项目中碰到一个elasticsearch的purge需求，就是说在不删除index和type的前提下，清除其中的所有数据。 用es的delete by query api可以做到，尽管官方声明deprecated in 1.5.3，但是经过测试1.8还是可以用的（真不敢想象要是不能用了怎么办。。。连个purge的api都没有。。），语法如下： 12345DELETE /&amp;lt;index&amp;gt;/&amp;lt;type&amp;gt;/\_query -d &apos;&#123; &quot;query&quot; : &#123; &quot;match_all&quot; : &#123;&#125; &#125;&#125;&apos; 简单来说就是根据查询条件查出所有符合条件的数据然后删掉，然后传入的查询条件是match_all。 注意DELETE /&lt;index&gt;/* 也可以删除所有数据，但相应的mapping也都没了，还不如重建索引。]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动修复Under-replicated Blocks]]></title>
    <url>%2F2015%2F09%2F19%2F2015-09-19_%E6%89%8B%E5%8A%A8%E4%BF%AE%E5%A4%8DUnder-replicated%20Blocks%2F</url>
    <content type="text"><![CDATA[切换到hdfs用户，否则会报Access denied1su - hdfs 将Under-replicated Blocks写入临时文件 1hdfs fsck / | grep &apos;Under replicated&apos; | awk -F&apos;:&apos; &apos;&#123;print $1&#125;&apos; &gt;&gt; /tmp/under_replicated_files 修改replication factor 1for hdfsfile in `cat /tmp/under_replicated_files`; do echo "Fixing $hdfsfile :" ; hadoop fs -setrep 3 $hdfsfile; done]]></content>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch Bulk API]]></title>
    <url>%2F2015%2F09%2F09%2F2015-09-09_Elasticsearch%20Bulk%20API%2F</url>
    <content type="text"><![CDATA[Elasticsearch Bulk API允许批量提交index和delete请求。 1234BulkRequestBuilder bulkRequest = client.prepareBulk(); bulkRequest.add(client.prepareIndex("index1", "type1", "id1").setSource(source); bulkRequest.add(client.prepareIndex("index2", "type2", "id2").setSource(source); BulkResponse bulkResponse = bulkRequest.execute().actionGet(); 但有时我们需要更精细的批量操控，比如 1234567891011121314151617181920212223BulkProcessor bulkProcessor = BulkProcessor.builder( client, new BulkProcessor.Listener() &#123; @Override public void beforeBulk(long executionId, BulkRequest request) &#123; ... &#125; @Override public void afterBulk(long executionId, BulkRequest request, BulkResponse response) &#123; ... &#125; @Override public void afterBulk(long executionId, BulkRequest request, Throwable failure) &#123; ... &#125; &#125;) .setBulkActions(10000) .setBulkSize(new ByteSizeValue(1, ByteSizeUnit.GB)) .setFlushInterval(TimeValue.timeValueSeconds(5)) .build(); bulkProcessor.add(new IndexRequest("index1", "type1", "id1").source(source1)); bulkProcessor.add(new DeleteRequest("index2", "type2", "id2"); beforeBulk会在批量提交之前执行，可以从BulkRequest中获取请求信息request.requests()或者请求数量request.numberOfActions()。 第一个afterBulk会在批量成功后执行，可以跟beforeBulk配合计算批量所需时间 第二个afterBulk会在批量失败后执行 在例子中，当请求超过10000个（default=1000）或者总大小超过1GB（default=5MB）时，触发批量提交动作。另外每隔5秒也会提交一次（默认不会根据时间间隔提交）。]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Proguard混淆代码导致Spring自动装配失败]]></title>
    <url>%2F2015%2F09%2F02%2F2015-09-02_Proguard%E6%B7%B7%E6%B7%86%E4%BB%A3%E7%A0%81%E5%AF%BC%E8%87%B4Spring%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[最近尝试用Proguard来混淆代码，以增加发布出去的代码安全性。今天运行混淆后的jar包发生如下异常 12345Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire field: public abc.service.Service test.rest.controller.Controller.service; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [abc.service.Service] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: &#123;@org.springframework.beans.factory.annotation.Autowired(required=true)&#125;at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:561)at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:88)at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:331)... 26 more 程序很简单，接口 123public interface Service &#123; int getAge(); &#125; 实现类使用@Component标签实例化到spring容器中 123456@Component public class ServiceImpl implements Service&#123; public int getAge() &#123; return new Random().nextInt(100); &#125; &#125; 在Controller中使用@Autowired标签自动装配 12@Autowired public Service service; 原程序可以正常运行，但在混淆了代码之后，发生以上所述的异常，原因是spring找不到能够匹配的Service实例。 忙活了一整天各种测试各种查资料，终于找到解决方案，很简单 在proguard配置中加上“-keepdirectories” 保证proguard版本不低于4.4 另外附上解决前和解决后的jar包解压时候的控制台输出 这是没加“-keepdirectories”的输出 123456inflated: META-INF/MANIFEST.MFinflated: abc/service/Service.classinflated: test/rest/controller/Controller.classinflated: test/rest/controller/ServiceImpl.classinflated: META-INF/maven/test/rest/pom.xmlinflated: META-INF/maven/test/rest/pom.properties 加上“-keepdirectories”之后解压 123456789101112131415created: META-INF/inflated: META-INF/MANIFEST.MFcreated: abc/created: abc/service/created: test/created: test/rest/created: test/rest/controller/inflated: abc/service/Service.classinflated: test/rest/controller/Controller.classinflated: test/rest/controller/ServiceImpl.classcreated: META-INF/maven/created: META-INF/maven/test/created: META-INF/maven/test/rest/inflated: META-INF/maven/test/rest/pom.xmlinflated: META-INF/maven/test/rest/pom.properties]]></content>
      <tags>
        <tag>proguard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala入门（四）：文件操作]]></title>
    <url>%2F2015%2F09%2F01%2F2015-09-01_Scala%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%20-%20%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[最近在做一个spark项目，顺便分享一下我的Scala入门过程。这一系列文章假定读者有一定的java或者其他面向对象编程语言基础。本文主要简单介绍文件的操作。 按惯例先上代码 12345678910111213141516val file = Source.fromFile("/Users/xiejing/Desktop/javascript") for (line &lt;- file.getLines()) &#123; println(line) &#125; file.close() val webFile = Source.fromURL("https://www.baidu.com/"); webFile.foreach(print) webFile.close var javaWriter = new BufferedWriter(new FileWriter(new File("/Users/xiejing/Desktop/a.txt"))) for (i &lt;- 4 to 10) &#123; javaWriter.write(i.toString()) &#125; javaWriter.flush() javaWriter.close() scala.io.Source.fromFile读取文件, getline获得一个iterator，遍历获取每一行,比Java方便多了。但这里有个问题，由于getline获得的是iterator，一旦你使用它完成遍历，iterator就失效了，所以如果需要反复的遍历文件，推荐Source.fromFile(…).getLines.toList，将文件内容转化为List，可以反复遍历，但代价是内存的消耗。 我们也可以用Source.fromURL来读取网络文件。 由于Scala运行在JVM上，因此可以无障碍的使用Java的类库，所以你可以使用java.io这个库来读取文件。]]></content>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala入门（三）：集合]]></title>
    <url>%2F2015%2F08%2F31%2F2015-08-31_Scala%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%20-%20%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[最近在做一个spark项目，顺便分享一下我的Scala入门过程。这一系列文章假定读者有一定的java或者其他面向对象编程语言基础。本文主要简单介绍Scala的集合类型，包括Array，Tuple和Map。 照例先上一段代码 12345678910111213141516171819object TestScala3 &#123; def main(args: Array[String]): Unit = &#123; val tuple = (1, "two", "three", true) println(tuple._1) println(tuple._2) val array = Array(1, "two", "three", true) for (i &lt;- 0 until array.length) &#123; println(array(i)) &#125; for (item &lt;- array) println(item) val map = Map(1 -&gt; "one", "two" -&gt; 2, true -&gt; "true") for ((k, v) &lt;- map) println("key = " + k + ", value = " + v) for ((_, v) &lt;- map) println("value = " + v) println(map.get(1)) println(map.get(3)) &#125; &#125; 第一个例子，我们创建了一个4个元素的tuple，并打印出它的前两个元素。tuple是一个元组，它可以包含若干个不同类型的元素（在Scala2.8之后，Array和List也支持不同类型的元素）。在取用tuple中的元素时，使用下划线加index的语法，但注意，与其他集合不同，tuple的index是从1开始的，实际上当你尝试写tuple._0时，会有得到编译错误“value _0 is not a member of (Int, String, String, Boolean)”， 并没有0这个下标。另外我们也从编译错误信息中看到，Scala具备类型推导的能力，它能够知道元组中的每个元素的类型。 在Java的函数中，如果我们想返回多个值，只能新建一个POJO去接，或者把返回值写入参数传入的引用中，非常麻烦，但在Scala中我们使用tuple就非常方便。 第二个例子，我们生命了一个Array，遍历并打印出Array中的每个元素。这里我们用到0until N，在上篇中我们写过0 to N，他们的区别在于until是一个前闭后开的函数，并不包括N，而to包括。 第三个例子，map储存的是键值对，用-&gt;声明。遍历时，把map中的每个键值对赋值给一个两个元素的tuple，再将这个tuple的两个元素打印出来。如果我们只关心map中的key或者value，我们可以使用占位符（_）来替代不关心的元素。]]></content>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala入门（二）：流程控制]]></title>
    <url>%2F2015%2F08%2F27%2F2015-08-27_%20Scala%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%20-%20%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[最近在做一个spark项目，顺便分享一下我的Scala入门过程。这一系列文章假定读者有一定的java或者其他面向对象编程语言基础。本文主要介绍Scala的流程控制和异常处理。 先上一段代码 12345678910111213141516171819202122232425object TestScala2 &#123; def main(args: Array[String]): Unit = &#123; //test 'for' for (i &lt;- 1 to 10) println(i) //test 'dowhile' var line = "" do &#123; line = StdIn.readLine() println(line) &#125; while (line != "") //test 'if else' val a = (Math.random() * 10).toInt; val result = if (a % 2 == 0) "even:" + a else "odd:" + a println(result) //test 'try catch finally' try &#123; //.... throw new Exception("--exception message--") &#125; catch &#123; case e: Exception =&gt; println("the exception is " + e.getMessage) &#125; finally &#123; println("finally") &#125; &#125; &#125; 我们先看第一行for循环的代码，它是做一个从1到10的循环，将遍历的每一个结果赋值给i，再将i打印出来。“&lt;-”是赋值符号，在Scala中非常常见。“1 to 10”表示1到10的循环，也可以写成“1.to(10)”，由于在Scala中一切皆对象，1是一个int类型的对象，它有to这个方法，并且接收另一个int对象作为参数。 在第二个例子中，将键盘的输入赋值给line并打印，如果line为空则跳出，否则继续循环接收输入。StdIn.readLine()表示接收键盘输入，StdIn这个类通过“import scala.io.StdIn”引入。while循环和dowhile循环类似。 第三个例子，判断一个随机数是奇数还是偶数，通过if else语句块实现，语法和Java类似，但有一点，Scala可以直接接收if else语句块的返回值。另外Math是java.lang这个包里的函数，Scala由于运行在jvm上，因此它可以直接调用Java。 最后一个例子是异常的处理。这里插一句，try catch finally语句块从语法上来说可以作为流程控制的手段，但从语义上来说，建议仅作为异常处理来使用，而不要用来控制流程的跳转，当然这是题外话了。我们看到在catch中，使用case来匹配捕获的异常，可以写若干个case，只要有一个能匹配上，即终止，不像Java中只要没有break就会继续匹配。]]></content>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala入门（一）：函数定义]]></title>
    <url>%2F2015%2F08%2F26%2F2015-08-26_Scala%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%20-%20%E5%87%BD%E6%95%B0%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[最近在做一个spark项目，顺便分享一下我的scala入门过程。这一系列文章假定读者有一定的java或者其他面向对象编程语言基础。本文主要介绍scala的函数定义。 按惯例，先来一段HelloWorld 123456789object TestScala1 &#123; def main(args: Array[String]): Unit = &#123; val s = "Hello World!"; show(s); &#125; def show(s: String): Unit = &#123; println(s) &#125; &#125; 函数的定义需要关键字def，组成部分有函数名（main），参数名称(args)， 参数类型(Array[String])，返回类型(Unit)，大括号内是函数体。 函数一般都会有返回内容，Unit为空返回值。Unit可以省略。实际上在接触scala时间长了之后你会发现，Scala是一门“能省则省”的语言。比如说每行语句末尾的分号，其实是可写可不写，或者准确的说，Scala是不推荐写分号的，因为没有意义，只是凭空增加了多敲一个键的负担（仁者见仁智者见智）。 和Java一样，main函数是程序的主入口，但不同的是，Java的main函数是一个类中的静态函数，但Scala的类中并没有静态函数，所以引入object来持有静态函数。所以请注意我们定义的TestScala1是一个object而不是class。 在main函数中我们首先声明一个变量s，val代表常量，var代表变量。由于Scala实现了类型推导，因此不需要声明对象的类型。 然后把s传递给函数show，函数的调用与Java类似，但如果函数没有参数的话，调用时可以省略括号（能省则省。。）]]></content>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark WARN TaskSchedulerImpl： Initial job has not accepted any resources]]></title>
    <url>%2F2015%2F08%2F06%2F2015-08-06_spark%20WARN%20TaskSchedulerImpl%EF%BC%9A%20Initial%20job%20has%20not%20accepted%20any%20resources%2F</url>
    <content type="text"><![CDATA[在本地提交一个spark job，出现如下错误 1WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory 在网上搜了一下，出现这种错误一般有两种原因， 内存不足。 主机名和IP配置不正确。 检查了一下，我只是跑一个简单的测试程序，内存是完全够用的，我也在spark web console上确认了内存是足够的。 为了确认主机名和IP的问题，我把SparkContext的内容打印了出来 123456spark.app.id=app-20150806145030-0003spark.app.name=Simple Applicationspark.cores.max=5spark.driver.host=192.168.56.1spark.driver.port=61685…… 这里显示我的IP是192.168.56.1，但实际上这是我的windows虚拟机的IP地址，而我本机的IP地址是192.168.100.151. 在提交spark job之后，server端需要反馈进度给driver host，所以反馈消息被发送到了虚拟机上而本机并没有收到，导致这个异常的发生。所以如果你的机器上装有虚拟机，在运行spark程序之前，最好用ifconfig down把虚拟机网卡关掉。]]></content>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rest service + spring boot + docker]]></title>
    <url>%2F2015%2F08%2F05%2F2015-08-05_rest%20service%20%2B%20spring%20boot%20%2B%20docker%2F</url>
    <content type="text"><![CDATA[开发 -&gt; 测试 -&gt; 部署，这是软件开发一般的简化流程，作为开发者我们总是希望能专注于开发，但往往会被一些开发之外的问题所折磨，比如繁多的spring依赖和不同的环境配置。带着这个问题，本文介绍运用springboot和docker开发和构建一个rest风格的web应用。 Spring 框架作为目前非常流行的一个 Java 应用开发框架，它包含几十个不同的子项目，涵盖应用开发的不同方面。要在这些子项目之间进行选择，并快速搭建一个可以运行的应用是比较困难的事情。Spring Boot 的目的在于快速创建可以独立运行的 Spring 应用，大大提升使用 Spring 框架时的开发效率。 对于我们的web应用来说，springboot相当于把web服务器嵌入发布包内，以少量的配置大大方便了程序的开发和发布。我们可以专注于项目本身，创建独立的Java应用，通过java -jar启动。 添加Spring Boot相关POM配置 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;version&gt;1.2.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 我们只需要将spring-boot-starter-web的依赖加入到pom文件即可，它提供了对web的支持。当然也有很多推荐让其作为parent加载，但这样会引入很多不必要的jar包。 Controller 12345678910111213141516@EnableAutoConfiguration @RestController @RequestMapping("/person") public class Controller &#123; @RequestMapping("/&#123;name&#125;") public Properties getPersion(@PathVariable("name") String name) &#123; Properties p = new Properties(); p.put("name", name); p.put("age", new Random().nextInt(100)); return p; &#125; public static void main(String[] args) &#123; SpringApplication.run(Controller.class); &#125; &#125; @EnableAutoConfiguration的作用在于让 Spring Boot 根据应用所声明的依赖来对 Spring 框架进行自动配置，这就减少了开发人员的工作量。比如说我们可以加入下面这段代码来制定contextpath和port。 12345@Bean public EmbeddedServletContainerFactory servletContainer() &#123; TomcatEmbeddedServletContainerFactory factory = new TomcatEmbeddedServletContainerFactory("/admin", 8091); return factory; &#125; 这样一个rest风格的web应用就搭建完成，可以访问http://localhost:8091/admin/person/kane查看结果。 Package 注意如果想到将应用打成一个可执行jar包的话，一定要用spring-boot-maven-plugin而不是其他的类似于maven-shade-plugin的插件。 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.2.5.RELEASE&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;mainClass&gt;test.controller.Controller&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; 直接用java -jar &lt;jar_name&gt;即可启动web应用。 Docker安装和运行 Docker 是一个构建、部署以及运行应用的开放平台，应用可以在不同的环境（开发、测试、生产）中无差别的运行，为应用的开发提供了很大的灵活性。 把可执行的jar包放入docker容器中，我们可以忽略所要部署的环境的差异性，而不需要关心环境配置。 Docker的安装和入门教程，推荐http://www.widuu.com/chinese_docker/userguide/dockerrepos.html，这里只是介绍最简单的流程。 对于ubuntu，可以用以下命令安装 $ sudo apt-get update$ sudo apt-get install docker.io$ sudo ln -sf /usr/bin/docker.io /usr/local/bin/docker$ sudo sed -i &#39;$acomplete -F _docker docker&#39; /etc/bash_completion.d/docker.io 选择一个java docker image，我使用java：7 sudo docker pull java:7 假设我们的可运行jar包放在/tmp目录下，那么可以通过以下命令在docker容器中运行。 sudo docker run -d -v /tmp:/tmp java:7 java -jar /tmp/&amp;lt;jar_name&amp;gt; 这个命令使用java:7这个镜像创建一个container，并在后台执行java -jar &lt;jar_name&gt;。-d表示后台运行，-v表示目录映射，可以简单理解为ln -s。 docker container启动起来之后,可以使用sudo docker ps命令查看该进程的端口映射，比如0.0.0.0:49155-&gt;8091/tcp，它表示将docker容器8091端口映射到容器宿主的49155端口，所以我们可以通过http://&lt;hostname&gt;:49155/admin/person/kane访问前文中的web应用。]]></content>
      <tags>
        <tag>springboot</tag>
        <tag>docker</tag>
        <tag>rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka实时监控]]></title>
    <url>%2F2015%2F07%2F28%2F2015-07-28_kafka%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[在kafka的开发和维护中，我们经常需要了解kafka topic以及连接在其上的consumer的实时信息，比如logsize，offset，owner等。为此kafka提供了ConsumerOffsetChecker，它的用法很简单 bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group &lt;group&gt; 输出结果类似于 Group Topic Pid Offset logSize Lag OwnerGroup1 a.topic 0 2 2 0 noneGroup1 a.topic 1 0 0 0 noneGroup1 a.topic 2 2 2 0 none 我们也可以通过kafka web console一类的工具直观地获取kafka信息，但如果我们要构建自己的监控系统，需要抓取这些信息的话，有两种办法：一种是运行ConsumerOffsetChecker然后解析输出的字符串；另一种就是通过SimpleConsumer和Zookeeper实时抓取信息（换句话说就是把ConsumerOffsetChecker翻译一下：）），以下介绍第二种方法的思路。 首先我们看kafka信息在zookeeper的存储结构 1，/brokers/topics/[topic]/partitions/[partitionId]/state2，/brokers/ids/[0…N]3，/consumers/[groupId]/ids/[consumerId]4，/consumers/[groupId]/owners/[topic]/[partitionId]5，/consumers/[groupId]/offsets/[topic]/[partitionId] 对于指定的topic和groupid，通过(1)可以拿到所有的partition信息（Pid），然后通过(5)可以拿到offset，通过(4)可以拿到owner。就差logsize还没法拿到，事实上logsize在zookeeper中并没有记录，它必须通过kafka consumer的low level的api取得。 1234567891011121314private Long getLogSize(String topicName, String partitionId, long offsetRequestTime) &#123; SimpleConsumer consumer = new SimpleConsumer(server, port, 10000, 1024000, "ConsumerOffsetChecker"); Long logSize = null; TopicAndPartition topicAndPartition = new TopicAndPartition(topicName, Integer.parseInt(partitionId)); Map&lt;TopicAndPartition, PartitionOffsetRequestInfo&gt; map = new HashMap&lt;TopicAndPartition, PartitionOffsetRequestInfo&gt;(); map.put(topicAndPartition, new PartitionOffsetRequestInfo(offsetRequestTime, 1)); OffsetRequest request = new OffsetRequest(map, kafka.api.OffsetRequest.CurrentVersion(), kafka.api.OffsetRequest.DefaultClientId()); OffsetResponse response = consumer.getOffsetsBefore(request); long[] aa = response.offsets(topicName, Integer.parseInt(partitionId)); if (aa.length != 0) &#123; logSize = aa[0]; &#125; return logSize; &#125; 第二行，创建simple consumer 第四行，创建topic和partition对象 第六行，创建offset request，第一个参数是记录写入的时间，如果是kafka.api.OffsetRequest.EarliestTime()，则代表当前最早的一条记录，也就是当前最小offset；如果是kafka.api.OffsetRequest.LatestTime()，则代表最新的一条记录，也就是当前最大offset。第二个参数是获取offset的个数。 由max offset和current offset，我们可以获得当前还有多少消息没有被消费（lag），由（lag/（maxoffset-minoffset）），我们可以算出当前还没有被消费的消息占的百分比，如果这个百分比接近100%，那么接下来很可能会导致offset out of range exception而丢失数据。]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase Rest API ： Hbase管理]]></title>
    <url>%2F2015%2F07%2F21%2F2015-07-21_Hbase%20Rest%20API%20%EF%BC%9A%20Hbase%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[在上一篇关于Hbase Rest API的文章中介绍了如何查询数据，本章将介绍其Hbase Admin的功能。 查询软件版本 语法：GET /version 范例：curl http://localhost:8000/version 输出结果： {“@Stargate”:”0.0.1”,”@OS”:”Linux 2.6.18-128.1.6.el5.centos.plusxen amd64”,”@JVM”:”Sun Microsystems Inc. 1.6.0_13-11.3-b02”,”@Jetty”:”6.1.14”,”@Jersey”:”1.1.0-ea”} 查询hbase集群版本 语法：GET /version/cluster 范例：curl -H &quot;Accept: application/json&quot; http://localhost:8000/version/cluster 输出结果： “0.20.0” 相当于HBaseAdmin.getClusterStatus().getHBaseVersion() 查询hbase集群状态 语法：GET /status/cluster 范例：curl -H &quot;Accept: application/json&quot; http://localhost:8000/status/cluster 相当于HBaseAdmin.getClusterStatus() 列出所有hbase表 语法：GET / 范例：curl -H &quot;Accept: application/json&quot; http://localhost:8000/ 输出结果： {“table”:[{“name”:”table1”},{“name”:”table2”}]} 相当于HBaseAdmin.listTableNames() 查询hbase表结构 语法：GET /&amp;lt;table&amp;gt;/schema 范例：curl -H &quot;Accept: application/json&quot; http://localhost:8000/table1/schema 输出结果： { NAME=&gt; ‘table1’, IS_META =&gt; ‘false’, COLUMNS =&gt; [ { NAME =&gt; ‘C’, DATA_BLOCK_ENCODING =&gt; ‘NONE’, BLOOMFILTER =&gt; ‘ROW’, REPLICATION_SCOPE =&gt; ‘0’, VERSIONS =&gt; ‘1’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt; ‘0’, TTL =&gt; ‘2147483647’, KEEP_DELETED_CELLS =&gt; ‘FALSE’, BLOCKSIZE =&gt; ‘65536’, IN_MEMORY =&gt; ‘false’, BLOCKCACHE =&gt; ‘true’ }, { NAME =&gt; ‘M’, DATA_BLOCK_ENCODING =&gt; ‘NONE’, BLOOMFILTER =&gt; ‘ROW’, REPLICATION_SCOPE =&gt; ‘0’, VERSIONS =&gt; ‘1’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt; ‘0’, TTL =&gt; ‘2147483647’, KEEP_DELETED_CELLS =&gt; ‘FALSE’, BLOCKSIZE =&gt; ‘65536’, IN_MEMORY =&gt; ‘false’, BLOCKCACHE =&gt; ‘true’ } ] } 相当于HBaseAdmin.getTableDescriptor(TableName.valueOf(“table1”)) 新建表或者更新表结构 语法:PUT /&amp;lt;table&amp;gt;/schemaPOST /&amp;lt;table&amp;gt;/schema PUT完全替换表结构，POST只是更新或者新增column family（跟本次更新无关的cf仍然保留） 查询region信息 语法：GET /&amp;lt;table&amp;gt;/regions 范例：curl -H &quot;Accept: application/json&quot; http://localhost:8000/table1/regions 输出结果： pf_entity,,1436346991071.fce2165af7b49f1dd0995ddd535d3a6f. [ id=1436346991071 startKey=’’ endKey=’’ location=’bd02:21003’] 相当于HBaseAdmin.getTableRegions(TableName.valueOf(“table1”))]]></content>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AspectJ+Javasist记录日志]]></title>
    <url>%2F2015%2F07%2F16%2F2015-07-16_AspectJ%2BJavasist%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[在项目中碰到这样一个需求，对一个服务类的每一个方法，在方法开始和结束的时候分别记录一条日志，内容包括方法名，参数名+参数值以及方法执行的时间。 12345678@Override public String get(String key) &#123; // long start = System.currentTimeMillis(); // System.out.println("Begin Method = get, Args = [key=" + key + "]"); String value = props.getProperty(key); // System.out.println("End Method = get, Args = [key=" + key + "], Cost=" + (System.currentTimeMillis() - start)+ "ms"); return value; &#125; 以上注释的代码能够满足要求，但是如果类中的方法比较多得时候，会有大量的冗余代码，不利于维护。这个时候可以用AspectJ实现日志代码的嵌入，但是JointPoint只能拿到参数值，无法拿到参数名。因此考虑用Javasist在初始化的时候先把参数名保存到内存中。以下是代码： 服务实现类（接口略）： 12345678910111213141516171819202122package test; import java.util.Properties; public class ServiceImpl implements Service &#123; private Properties props = new Properties(); @Override public String get(String key) &#123; return props.getProperty(key); &#125; @Override public void put(String key, String value) &#123; props.put(key, value); &#125; @Override public void putAll(Properties props) &#123; this.props.putAll(props); &#125; &#125; Aspect类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101package test; import java.util.Map; import javassist.ClassClassPath; import javassist.ClassPool; import javassist.CtClass; import javassist.CtMethod; import javassist.Modifier; import javassist.NotFoundException; import javassist.bytecode.CodeAttribute; import javassist.bytecode.LocalVariableAttribute; import javassist.bytecode.MethodInfo; import org.apache.log4j.Logger; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Pointcut; import com.google.common.collect.Maps; @Aspect public class LogHelper &#123; final static private Logger logger = Logger.getLogger(ServiceImpl.class); private static final String DELIMITER = " | "; private Map&lt;String, String[]&gt; paraMap = Maps.newHashMap(); public void init() throws NotFoundException &#123; reflectGetParamName(ServiceImpl.class); &#125; @Pointcut("execution(* (test.*+).*(..))") public void aspectjMethod() &#123; &#125; @Around("aspectjMethod()") public Object around(ProceedingJoinPoint call) throws Throwable &#123; long startTime = System.nanoTime(); String methodName = call.getSignature().getName(); String className = call.getTarget().getClass().getSimpleName(); Object[] args = call.getArgs(); StringBuilder begin = new StringBuilder("Begin ").append("Method=").append(methodName).append(", Args=[") .append(argsToString(className, methodName, args)).append("]"); logger.info(begin); Object result = call.proceed(args); long costTime = (System.nanoTime() - startTime) / 1000;// μs StringBuilder end = new StringBuilder("End ").append("Method=").append(methodName).append(", Args=[") .append(argsToString(className, methodName, args)).append("], Cost=").append(costTime).append("μs"); logger.info(end); return result; &#125; private String argsToString(String className, String methodName, Object[] args) &#123; String[] paraNames = paraMap.get(className + methodName); boolean appendParaName = true; if (paraNames == null || paraNames.length != args.length) &#123; logger.warn("Fail to get parameter(s)' name. ClassName = " + className + ", MethodName = " + methodName + ", Expected ParaNum = " + args.length); appendParaName = false; &#125; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; args.length; i++) &#123; Object obj = args[i]; String paraName = paraNames[i]; if (sb.length() != 0) &#123; sb.append(DELIMITER); &#125; if (appendParaName) &#123; sb.append(paraName).append("="); &#125; sb.append(String.valueOf(obj)); &#125; return sb.toString(); &#125; private void reflectGetParamName(Class&lt;?&gt; clazz) throws NotFoundException &#123; ClassPool pool = ClassPool.getDefault(); pool.insertClassPath(new ClassClassPath(clazz)); CtClass cc = pool.get(clazz.getName()); for (CtMethod cm : cc.getDeclaredMethods()) &#123; String methodName = cm.getName(); // 使用javaassist的反射方法获取方法的参数名 MethodInfo methodInfo = cm.getMethodInfo(); CodeAttribute codeAttribute = methodInfo.getCodeAttribute(); LocalVariableAttribute attr = (LocalVariableAttribute) codeAttribute .getAttribute(LocalVariableAttribute.tag); if (attr == null) &#123; throw new NotFoundException("CodeAttribute Not Found. ClassName = " + clazz.getName() + ", MethodName = " + methodName); &#125; String[] paramNames = new String[cm.getParameterTypes().length]; int pos = Modifier.isStatic(cm.getModifiers()) ? 0 : 1; for (int i = 0; i &lt; paramNames.length; i++) paramNames[i] = attr.variableName(i + pos); paraMap.put(clazz.getSimpleName() + methodName, paramNames); &#125; &#125; &#125; 首先初始化的时候调用reflectGetParamName，运用反射获取所有方法的参数名，并保存到paraMap中。 在方法执行的前后分别打印日志并计算执行时间，方法名和参数值从JoinPoint中获取，参数名从paraMap中获取 配置文件applicationContext.xml： 123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.0.xsd"&gt; &lt;!-- 启用AspectJ对Annotation的支持 --&gt; &lt;aop:aspectj-autoproxy/&gt; &lt;bean id="service" class="test.ServiceImpl"/&gt; &lt;bean id="aspcejHandler" class="test.LogHelper" init-method="init"/&gt; &lt;/beans&gt; 测试代码： 1234567891011121314151617package test; import java.util.Properties; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; public class Test &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext("applicationContext.xml"); Service service = (Service) context.getBean("service"); service.put("1", "one"); service.putAll(new Properties()); System.out.println(service.get("1")); &#125; &#125; 测试结果： 15/07/16 10:19:38 INFO test.ServiceImpl: Begin Method=put, Args=[key=1 | value=one]15/07/16 10:19:38 INFO test.ServiceImpl: End Method=put, Args=[key=1 | value=one], Cost=1539μs15/07/16 10:19:38 INFO test.ServiceImpl: Begin Method=putAll, Args=[props={}]15/07/16 10:19:38 INFO test.ServiceImpl: End Method=putAll, Args=[props={}], Cost=148μs15/07/16 10:19:38 INFO test.ServiceImpl: Begin Method=get, Args=[key=1]15/07/16 10:19:38 INFO test.ServiceImpl: End Method=get, Args=[key=1], Cost=86μs]]></content>
      <tags>
        <tag>AspectJ</tag>
        <tag>Javasist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase Rest API ： 数据查询]]></title>
    <url>%2F2015%2F07%2F13%2F2015-07-13_Hbase%20Rest%20API%20%EF%BC%9A%20%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[hbase（hadoop）是用java编写的，有些语言（例如python）能够对它提供良好的支持，但也有很多语言使用起来并不是那么方便，比如c#只能通过thrift访问。Rest就能很好的解决这个问题。Hbase的org.apache.hadoop.hbase.rest包提供了rest接口，它内嵌了jetty作为servlet容器。 启动命令：./bin/hbase rest start -p &lt;port&gt; 默认端口8080. 以下介绍如何使用rest api查询hbase数据。 rowkey查询 单值查询。 只查询单个column family或column。 语法：GET /&amp;lt;table&amp;gt;/&amp;lt;row&amp;gt;/&amp;lt;column&amp;gt;(:&amp;lt;qualifier&amp;gt; )?(/&amp;lt;timestamp&amp;gt;)? 范例：curl -H &quot;Accept: application/json&quot; http://localhost:8000/table/key/cf:raw 在测试中我发现加入&lt;timestamp&gt;参数之后就找不到数据了，不知道是不是bug 多值查询。 查询多个column family或column。 语法：GET /&amp;lt;table&amp;gt;/&amp;lt;row&amp;gt; ( / ( &amp;lt;column&amp;gt; ( : &amp;lt;qualifier&amp;gt; )? ( , &amp;lt;column&amp;gt; ( : &amp;lt;qualifier&amp;gt; )? )+ )?( / ( &amp;lt;start-timestamp&amp;gt; &#39;,&#39; )? &amp;lt;end-timestamp&amp;gt; )? )?( ?v= &amp;lt;num-versions&amp;gt; )? 范例：curl -H &quot;Accept: application/json&quot; http://localhost:8000/table1/key1/cf1:q1,cf2/1436350580000,1436350590000/3 可以通过timestamp过滤查询结果，也可以通过&lt;num-versions&gt;设定返回的最大版本数 通配符查询 在rowkey后添加*号，可以返回以该rowkey为前缀的所有记录，如果以*号为rowkey来查询，则返回该table中所有的记录。 范例：curl -H &quot;Accept: application/json&quot; http://localhost:8000/table/keyprefix*/cf key list查询 据我所知，暂不支持该功能，只能对循环key list对每个key做查询。 key范围查询 有状态scanner 首先在server端创建scanner:PUT /&lt;table&gt;/scanner 范例：curl -H &quot;Content-Type: text/xml&quot; -d &#39;&amp;lt;Scanner batch=&quot;1&quot;/&amp;gt;&#39; http://localhost:8000/table/scanner 在response中获得scanner id，然后可以使用这个id拿到这个scanner查询到的所有cell的值， 语法：GET /&amp;lt;table&amp;gt;/scanner/&amp;lt;scanner-id&amp;gt; 范例：curl -H &quot;Content-Type: application/json&quot; http://localhost:8000/table/scanner/12447063229213b1937 注意每次调用只能返回一个cell的值，而且比较奇怪的是只返回值而没有cf和qualifier，所以使用起来很不方便。 使用完之后删除scanner:DELETE /&lt;table&gt;/scanner/&lt;scanner-id&gt; 无状态scanner 无状态的scanner不保存任何关于查询的状态，它把所有的查询条件作为参数进行一次性的查询。 查询参数包括： startrow - 查询起始keyendrow - 查询终止keycolumns - 查询的columnstarttime, endtime - 通过指定起始时间选择特定的数据版本，starttime和endtime必须同时设置maxversions - 每个返回的cell的版本数limit - 返回数据条数 范例： 查询单个column cf:q：curl -H &quot;Content-Type: text/xml&quot; https://localhost:8080/table/*?columns=cf:q 查询多个column cf1:q1和cf2:q2：curl -H &quot;Content-Type: text/xml&quot; https://localhost:8080/table/*?columns=cf1:q1,cf2:q2 从key1开始查询两条记录：curl -H &quot;Content-Type: text/xml&quot; https://localhost:8080/table/*?startrow=key1&amp;limit=2 查询1389900769772和1389900800000之间的版本：curl -H &quot;Content-Type: text/xml&quot; https://localhost:8080/table/*？starttime=1389900769772&amp;endtime=1389900800000]]></content>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring boot内嵌的tomcat启动失败]]></title>
    <url>%2F2015%2F07%2F10%2F2015-07-10_Spring%20boot%E5%86%85%E5%B5%8C%E7%9A%84tomcat%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[根据这篇guide创建了一个简单的spring boot应用，能运行且成功的访问。但移植到现有项目（基于hbase）中的时候，却报出以下错误： 12345678910111213141516171819202122232425SEVERE: A child container failed during start java.util.concurrent.ExecutionException: org.apache.catalina.LifecycleException: Failed to start component [StandardEngine[Tomcat].StandardHost[localhost].StandardContext[]] at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:188) at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:1123) at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:801) at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150) at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1559) at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1549) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Caused by: org.apache.catalina.LifecycleException: Failed to start component [StandardEngine[Tomcat].StandardHost[localhost].StandardContext[]] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:154) ... 6 more Caused by: java.lang.NoSuchMethodError: javax.servlet.ServletContext.addServlet(Ljava/lang/String;Ljavax/servlet/Servlet;)Ljavax/servlet/ServletRegistration$Dynamic; at org.springframework.boot.context.embedded.ServletRegistrationBean.onStartup(ServletRegistrationBean.java:156) at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext$1.onStartup(EmbeddedWebApplicationContext.java:219) at org.springframework.boot.context.embedded.tomcat.ServletContextInitializerLifecycleListener.lifecycleEvent(ServletContextInitializerLifecycleListener.java:54) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119) at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:90) at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5343) at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150) ... 6 more 一般碰到NoSuchMethodError，要么是没有引入dependency，要么是jar包冲突。在这里应该是servlet-api.jar有冲突。 用命令mvn dependency:tree查看项目所有dependencies，发现有以下依赖： 12345678[INFO] | +- org.apache.hbase:hbase-server:jar:0.96.1.1-cdh5.0.2:compile...[INFO] | | +- org.mortbay.jetty:servlet-api-2.5:jar:6.1.14:compile...[INFO] | | +- org.apache.hadoop:hadoop-common:jar:2.3.0-cdh5.0.2:compile...[INFO] | | | +- javax.servlet:servlet-api:jar:2.5:compile... hbase-server中也有servlet-api的jar包，尝试将其移除： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;0.96.1.1-cdh5.0.2&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt; &lt;artifactId&gt;servlet-api-2.5&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; 运行成功。]]></content>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka consumer防止数据丢失]]></title>
    <url>%2F2015%2F07%2F07%2F2015-07-07_kafka%20consumer%E9%98%B2%E6%AD%A2%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%2F</url>
    <content type="text"><![CDATA[kafka最初是被LinkedIn设计用来处理log的分布式消息系统，因此它的着眼点不在数据的安全性（log偶尔丢几条无所谓），换句话说kafka并不能完全保证数据不丢失。 尽管kafka官网声称能够保证at-least-once，但如果consumer进程数小于partition_num，这个结论不一定成立。 考虑这样一个case，partiton_num=2，启动一个consumer进程订阅这个topic，对应的，stream_num设为2，也就是说启两个线程并行处理message。 如果auto.commit.enable=true，当consumer fetch了一些数据但还没有完全处理掉的时候，刚好到commit interval出发了提交offset操作，接着consumer crash掉了。这时已经fetch的数据还没有处理完成但已经被commit掉，因此没有机会再次被处理，数据丢失。 如果auto.commit.enable=false，假设consumer的两个fetcher各自拿了一条数据，并且由两个线程同时处理，这时线程t1处理完partition1的数据，手动提交offset，这里需要着重说明的是，当手动执行commit的时候，实际上是对这个consumer进程所占有的所有partition进行commit，kafka暂时还没有提供更细粒度的commit方式，也就是说，即使t2没有处理完partition2的数据，offset也被t1提交掉了。如果这时consumer crash掉，t2正在处理的这条数据就丢失了。 如果希望能够严格的不丢数据，解决办法有两个： 手动commit offset，并针对partition_num启同样数目的consumer进程，这样就能保证一个consumer进程占有一个partition，commit offset的时候不会影响别的partition的offset。但这个方法比较局限，因为partition和consumer进程的数目必须严格对应。 另一个方法同样需要手动commit offset，另外在consumer端再将所有fetch到的数据缓存到queue里，当把queue里所有的数据处理完之后，再批量提交offset，这样就能保证只有处理完的数据才被commit。当然这只是基本思路，实际上操作起来不是这么简单，具体做法以后我再另开一篇。 ———————————- 更新 ——————————————– Kafka在0.9版本重新设计的Consumer解决了这个问题,解决的方法是…………..不允许在多个线程中使用同一个Consumer实例,服气服气]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka offset迁移]]></title>
    <url>%2F2015%2F07%2F05%2F2015-07-05_kafka%20offset%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[在早前的kafka版本中（0.8.0），offset是被存储在zookeeper中的。 到当前版本（0.8.2）为止，kafka同时支持offset存储在zookeeper和offset manager（broker）中。 从官方的说明来看，未来offset的zookeeper存储将会被弃用。因此现有的基于kafka的项目如果今后计划保持更新的话，可以考虑在合适的时候将offset迁移到kafka broker上。 以下是迁移步骤： 在consume config中，修改offsets.storage=kafka并且dual.commit.enabled=true。第一个修改不用解释。第二个配置项的意思是，同时提交offset到zookeeper和offset manager上，这是为了保证在迁移过程中offset不会丢失。 rolling restart consumers并且确认运行正常。这时已经完成offset的迁移工作。 修改consumer config，dual.commit.enabled=false。双重提交offset会带来额外的开销，在完成迁移工作之后最好把这项配置关闭。 rolling restart consumers]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastjson初始化对性能的影响]]></title>
    <url>%2F2015%2F07%2F04%2F2015-07-04_fastjson%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[之前在项目中序列化是用thrift，性能一般，而且需要用编译器生成新的类，在序列化和反序列化的时候感觉很繁琐，因此想转到json阵营。对比了jackson，gson等框架之后，决定用fastjson，为什么呢，因为看名字感觉很快。。。 网上的说法： fastjson 是一个性能很好的 Java 语言实现的 JSON 解析器和生成器，来自阿里巴巴的工程师开发。 主要特点： 快速FAST (比其它任何基于Java的解析器和生成器更快，包括jackson） 强大（支持普通JDK类包括任意Java Bean Class、Collection、Map、Date或enum） 零依赖（没有依赖其它任何类库除了JDK） 但是测试的结果让我大跌眼镜。 Test Case: 对一个User类（空）的对象分别用java和fastjson序列化1000次，再反序列化1000次，计算时间。注： 测试不是非常严谨，只是做个简单的比较。 Test Result： Type = javaSerialize cost = 27msDeserialize cost = 75ms Type = fastjsonSerialize cost = 385msDeserialize cost = 84ms 这是在逗我吗。。。 经过同事提醒，看了源码发现fastjson在序列化时需要初始化SerializeConfig，反序列化时需要初始化ParserConfig。然后我在测试案例中加入这两句 12public static ParserConfig pc = new ParserConfig(); public static SerializeConfig sc = new SerializeConfig(); 果然快了很多，但仍旧不理想 Type = fastjsonSerialize cost = 36msDeserialize cost = 42ms 再继续看，发现还需要初始化writer和parser，所以改成这句 1JSON.parseObject(JSON.toJSONString(new User()), User.class); 结果就很令人满意了 Type = fastjsonSerialize cost = 15msDeserialize cost = 18ms 结论： 如果你使用fastjson在一个短进程，换句话说只是少量的进行序列化反序列化，那么fastjson由于初始化需要的时间比较长，总体性能将会很糟糕。如果一定要用，有必要的话可以考虑手动进行初始化。 另，补上测试代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576class User implements Serializable &#123; private static final long serialVersionUID = -2513747641863637392L; User() &#123; &#125; &#125; public class Test &#123; // public static ParserConfig pc = new ParserConfig(); // public static SerializeConfig sc = new SerializeConfig(); public static void main(String[] args) throws UnknownHostException &#123; // JSON.parseObject(JSON.toJSONString(new User()), User.class); String type = "json"; System.out.println("Type = " + type); long start = new Date().getTime(); byte[] b = serialize(new User(), type); long mid = new Date().getTime(); System.out.println("Serialize cost = " + (mid - start)); deserialize(b, type); System.out.println("Deserialize cost = " + (new Date().getTime() - mid)); &#125; public static byte[] serialize(User user, String type) &#123; byte[] b = null; for (int i = 0; i &lt; 1000; i++) &#123; if ("java".equalsIgnoreCase(type)) &#123; b = javaSerialize(user); &#125; else if ("json".equalsIgnoreCase(type)) &#123; b = jsonSerialize(user); &#125; &#125; return b; &#125; public static User deserialize(byte[] b, String type) &#123; User user = null; for (int i = 0; i &lt; 1000; i++) &#123; if ("java".equalsIgnoreCase(type)) &#123; user = javaDeserialize(b); &#125; else if ("json".equalsIgnoreCase(type)) &#123; user = jsonDeserialize(b); &#125; &#125; return user; &#125; public static byte[] jsonSerialize(User user) &#123; return JSON.toJSONString(user).getBytes(); &#125; public static User jsonDeserialize(byte[] b) &#123; return JSON.parseObject(new String(b), User.class); &#125; public static byte[] javaSerialize(User user) &#123; try &#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); ObjectOutputStream os = new ObjectOutputStream(out); os.writeObject(user); return out.toByteArray(); &#125; catch (Exception e) &#123; throw new SerializationException(e); &#125; &#125; public static User javaDeserialize(byte[] b) &#123; try &#123; ByteArrayInputStream in = new ByteArrayInputStream(b); ObjectInputStream is = new ObjectInputStream(in); return (User) is.readObject(); &#125; catch (Exception e) &#123; throw new SerializationException(e); &#125; &#125; &#125;]]></content>
      <tags>
        <tag>fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Rest ： Confluent]]></title>
    <url>%2F2015%2F07%2F02%2F2015-07-02_Kafka%20Rest%2F</url>
    <content type="text"><![CDATA[最近拿到一个kafka rest的需求，但kafka暂时还没有提供rest api（应该是有在开发中，毕竟rest这么火），上网搜了一下，找到一个Confluent Platform，本文简单介绍一下安装。 这里插一句，给大家推荐一个九尾搜索，原名叫谷粉SOSO，不想fanqiang谷歌的可以用这个。以前在外企用谷歌用习惯了，出来之后用度娘搜技术问题，那匹配度简直感人。 环境声明：Ubuntu 4.8.2, Java version “1.7.0_55”,已经安装zookeeper和kafka 安装Confluent Platform 官网提供了几种下载安装的方法，我是直接在本地下载然后拷贝到linux机器上 http://packages.confluent.io/archive/1.0/confluent-1.0-2.10.4.tar.gz 然后解压 tar xzf confluent-1.0-2.10.4.tar.gzcd confluent-1.0 编辑etc/kafka-rest/kafka-rest.properties， 修改zookeeper.connect为你的kafka集群的zookeeper地址 启动kafka restserve : bin/kafka-rest-start 测试 ： curl “http://localhost:8082/topics&quot;]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带filter的hbase查询优化]]></title>
    <url>%2F2015%2F07%2F01%2F2015-07-01_%E5%B8%A6filter%E7%9A%84hbase%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[问题描述 hbase scan数据缓慢，server端出现LeaseException。hbase写入缓慢。 问题原因 直接原因是： hbase client端每次和regionserver交互的时候，都会在服务器端生成一个Lease,Lease的有效期由参数hbase.regionserver.lease.period确定。如果hbase scan需要的时间超过hbase.regionserver.lease.period所配置的时间，并且客户端没有和regionserver报告其还活着，那么regionserver就会认为Lease已经过期，并从LeaseQueue中删除，当regionserver加载完成后，拿已经被删除的Lease再去取数据的时候，就会出现LeaseException。 而根本原因在于单个查询请求在server端处理时间过长，经过检查，发现是因为该scan带有一个会把绝大多数数据过滤掉的filter，导致hbase扫描了几百万条数据也没能拿到指定数量（scan的caching）的数据。 这和全表扫描还不一样。由于有查询缓存，即使是全表扫描，server端在拿到满足缓存需求的数据之后就会返回结果，实际上单个请求并不会扫描整张表。 相关影响 想象client端发出一个查询请求之后一分多钟没有收到response，用户体验非常的糟糕 一个请求会占用一个hbase的handler，假设handler数量是100，而client端同时发过来100个这样的带filter的查询请求，那么可以预见，所有的handler将会在这些查询进行中被hold住，导致其他请求（包括查询请求和写入请求）不能得到及时的处理，只能等待。实际上在发生这个问题的时候，最先发现的状况就是写入效率异常的慢。 解决方案 网上有些朋友给出的意见是增大hbase.regionserver.lease.period以减少LeaseException，这我是同意的。 还有的意见是减小hbase.rpc.timeout，这样client端就能很快得到反馈。这样用户体验确实好了一点，但并没有实际解决问题。。。server端的查询实际上还在跑啊！handler还是没有释放啊！况且一直抛timeout exception给client也不是办法，这个请求永远没有办法处理。 我们给出的解决方案是，用RandomRowFilter。 123456FilterList filterList = new FilterList(Operator.MUST_PASS_ONE); // RandomRowFilter must on the first of the FilterList filterList.addFilter(new RandomRowFilter(0.01F)); filterList.addFilter(...);//add other filters Scan scan = new Scan(); scan.setFilter(filterList); 它的原理很简单，hbase每扫描过一条记录，就会生成一个随机数，如果这个随机数小于某个阈值，就终止这次查询并返回已经查询到的结果。那么这个阈值如何定义呢？ 12Random random = new Random(); boolean filterOutRow = !(random.nextFloat() &lt; chance); 这个chance就是阈值，也就是上面例子中的构造参数0.01F。因此你可以根据需要设定chance的大小，来控制扫描的最大记录数。 因此假设我们设chance=0.01F，在Scan的时候即使使用了一个很糟糕的filter，server端在扫描了100条记录（数字非确定，以下会说明）后就会终止，并返回当前扫描到的记录，如果需要继续进行查询，使用当前记录的rowkey作为startkey再发起一次查询请求即可。 但有几点需要注意： RandomRowFilter只是表达概率而不是确定值。比如设chance=0.01F，并不能保证一定会扫描100条记录就停止，而只是到100就停止的概率最大，但有可能只扫描一条记录，也有可能是一万条。但多次重复取平均值的话，肯定无限趋近于100。 带RandomRowFilter的查询返回的结果不一定是我们需要的结果。如果只是由于RandomRowFilter扫描终止，会把当前扫描到的那条记录也返回，无论这条记录是不是符合其他filter的要求。我们的做法是检查最后一条记录，看是否符合所有filter的条件。]]></content>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
</search>
